{"meta":{"title":"雪山上的蒲公英","subtitle":"JinFeng's Blog","description":"Stay hungry, Stay foolish.","author":"Zang JinFeng","url":"https://xssdpgy.github.io","root":"/"},"pages":[{"title":"关于","date":"2022-04-07T06:04:21.000Z","updated":"2022-09-11T15:43:16.282Z","comments":false,"path":"about/index.html","permalink":"https://xssdpgy.github.io/about/index.html","excerpt":"","text":"程序员一枚，专向互金工程领域。"},{"title":"分类","date":"2022-04-06T15:33:39.000Z","updated":"2022-09-11T15:43:16.282Z","comments":false,"path":"categories/index.html","permalink":"https://xssdpgy.github.io/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2022-04-06T15:31:49.000Z","updated":"2022-09-11T15:43:16.283Z","comments":false,"path":"tags/index.html","permalink":"https://xssdpgy.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Java的7种阻塞队列及其实现原理","slug":"Java的7种阻塞队列及其实现原理","date":"2023-02-27T14:50:55.000Z","updated":"2023-02-28T14:16:22.373Z","comments":true,"path":"2023/02/27/Java的7种阻塞队列及其实现原理/","link":"","permalink":"https://xssdpgy.github.io/2023/02/27/Java%E7%9A%847%E7%A7%8D%E9%98%BB%E5%A1%9E%E9%98%9F%E5%88%97%E5%8F%8A%E5%85%B6%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/","excerpt":"队列和阻塞队列队列队列（Queue）是一种经常使用的集合。Queue 实际上是实现了一个先进先出（FIFO：First In First Out）的有序表。和 List、Set 一样都继承自 Collection。它和 List 的区别在于，List可以在任意位置添加和删除元素，而Queue 只有两个操作： 把元素添加到队列末尾； 从队列头部取出元素。","text":"队列和阻塞队列队列队列（Queue）是一种经常使用的集合。Queue 实际上是实现了一个先进先出（FIFO：First In First Out）的有序表。和 List、Set 一样都继承自 Collection。它和 List 的区别在于，List可以在任意位置添加和删除元素，而Queue 只有两个操作： 把元素添加到队列末尾； 从队列头部取出元素。 超市的收银台就是一个队列： 我们常用的 LinkedList 就可以当队列使用，实现了 Dequeue 接口，还有 ConcurrentLinkedQueue，他们都属于非阻塞队列。 阻塞队列阻塞队列，顾名思义，首先它是一个队列，而一个阻塞队列在数据结构中所起的作用大致如下： 线程 1 往阻塞队列中添加元素，而线程 2 从阻塞队列中移除元素 当阻塞队列是空时，从队列中获取元素的操作将会被阻塞。 当阻塞队列是满时，从队列中添加元素的操作将会被阻塞。 试图从空的阻塞队列中获取元素的线程将会阻塞，直到其他的线程往空的队列插入新的元素，同样，试图往已满的阻塞队列添加新元素的线程同样也会阻塞，直到其他的线程从列中移除一个或多个元素或者完全清空队列后继续新增。 类似我们去海底捞排队，海底捞爆满情况下，阻塞队列相当于用餐区，用餐区满了的话，就阻塞在候客区等着，可以用餐的话 put 一波去用餐，吃完就 take 出去。 为什么要用阻塞队列在多线程领域：所谓阻塞，是指在某些情况下会挂起线程（即阻塞），一旦条件满足，被挂起的线程又会自动被唤醒。 那为什么需要 BlockingQueue 呢 好处是我们不需要关心什么时候需要阻塞线程，什么时候需要唤醒线程，因为这些 BlockingQueue 都包办了。 在 concurrent 包发布以前，多线程环境下，我们每个程序员都必须自己去实现这些细节，尤其还要兼顾效率和线程安全，这会给我们的程序带来不小的复杂性。现在有了阻塞队列，我们的操作就从手动挡换成了自动挡。 Java里的阻塞队列 Collection的子类除了我们熟悉的 List 和 Set，还有一个 Queue，阻塞队列 BlockingQueue 继承自 Queue。 BlockingQueue 是个接口，需要使用它的实现之一来使用 BlockingQueue，java.util.concurrent 包下具有以下 BlockingQueue 接口的实现类： JDK 提供了 7 个阻塞队列。分别是： ArrayBlockingQueue ：一个由数组结构组成的有界阻塞队列 LinkedBlockingQueue ：一个由链表结构组成的有界阻塞队列 PriorityBlockingQueue ：一个支持优先级排序的无界阻塞队列 DelayQueue：一个使用优先级队列实现的无界阻塞队列 SynchronousQueue：一个不存储元素的阻塞队列 LinkedTransferQueue：一个由链表结构组成的无界阻塞队列（实现了继承于 BlockingQueue 的 TransferQueue） LinkedBlockingDeque：一个由链表结构组成的双向阻塞队列 BlockingQueue核心方法相比 Queue 接口，BlockingQueue 有四种形式的 API。 方法类型 抛出异常 返回特殊值 一直阻塞 超时退出 插入 add(e) offer(e) put(e) offer(e,time,unit) 移除（取出） remove() poll() take() poll(time,unit) 检查 element() peek() 不可用 不可用 以 ArrayBlockingQueue 为例来看下 Java 阻塞队列提供的常用方法 抛出异常： 当阻塞队列满时，再往队列里 add 插入元素会抛出 java.lang.IllegalStateException: Queue full 异常； 当队列为空时，从队列里 remove 移除元素时会抛出 NoSuchElementException 异常 。 element()，返回队列头部的元素，如果队列为空，则抛出一个 NoSuchElementException 异常 返回特殊值： offer()，插入方法，成功返回 true，失败返回 false； poll()，移除方法，成功返回出队列的元素，队列里没有则返回 null peek() ，返回队列头部的元素，如果队列为空，则返回 null 一直阻塞： 当阻塞队列满时，如果生产线程继续往队列里 put 元素，队列会一直阻塞生产线程，直到拿到数据，或者响应中断退出； 当阻塞队列空时，消费线程试图从队列里 take 元素，队列也会一直阻塞消费线程，直到队列可用。 超时退出： 当阻塞队列满时，队列会阻塞生产线程一定时间，如果超过一定的时间，生产线程就会退出，返回 false 当阻塞队列空时，队列会阻塞消费线程一定时间，如果超过一定的时间，消费线程会退出，返回 null BlockingQueue 实现类逐个分析下这 7 个阻塞队列，常用的几个顺便探究下源码。 ArrayBlockingQueueArrayBlockingQueue，一个由数组实现的有界阻塞队列。该队列采用先进先出（FIFO）的原则对元素进行排序添加的。 ArrayBlockingQueue 为有界且固定，其大小在构造时由构造函数来决定，确认之后就不能再改变了。 ArrayBlockingQueue 支持对等待的生产者线程和使用者线程进行排序的可选公平策略，但是在默认情况下不保证线程公平的访问，在构造时可以选择公平策略（fair = true）。公平性通常会降低吞吐量，但是减少了可变性和避免了“不平衡性”。（ArrayBlockingQueue 内部的阻塞队列是通过 ReentrantLock 和 Condition 条件队列实现的， 所以 ArrayBlockingQueue 中的元素存在公平和非公平访问的区别） 所谓公平访问队列是指阻塞的所有生产者线程或消费者线程，当队列可用时，可以按照阻塞的先后顺序访问队列，即先阻塞的生产者线程，可以先往队列里插入元素，先阻塞的消费者线程，可以先从队列里获取元素，可以保证先进先出，避免饥饿现象。 源码解读 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208public class ArrayBlockingQueue&lt;E&gt; extends AbstractQueue&lt;E&gt; implements BlockingQueue&lt;E&gt;, java.io.Serializable &#123; // 通过数组来实现的队列 final Object[] items; //记录队首元素的下标 int takeIndex; //记录队尾元素的下标 int putIndex; //队列中的元素个数 int count; //通过ReentrantLock来实现同步 final ReentrantLock lock; //有2个条件对象，分别表示队列不为空和队列不满的情况 private final Condition notEmpty; private final Condition notFull; //迭代器 transient Itrs itrs; //offer方法用于向队列中添加数据 public boolean offer(E e) &#123; // 可以看出添加的数据不支持null值 checkNotNull(e); final ReentrantLock lock = this.lock; //通过重入锁来实现同步 lock.lock(); try &#123; //如果队列已经满了的话直接就返回false，不会阻塞调用这个offer方法的线程 if (count == items.length) return false; else &#123; //如果队列没有满，就调用enqueue方法将元素添加到队列中 enqueue(e); return true; &#125; &#125; finally &#123; lock.unlock(); &#125; &#125; //多了个等待时间的 offer方法 public boolean offer(E e, long timeout, TimeUnit unit) throws InterruptedException &#123; checkNotNull(e); long nanos = unit.toNanos(timeout); final ReentrantLock lock = this.lock; //获取可中断锁 lock.lockInterruptibly(); try &#123; while (count == items.length) &#123; if (nanos &lt;= 0) return false; //等待设置的时间 nanos = notFull.awaitNanos(nanos); &#125; //如果等待时间过了，队列有空间的话就会调用enqueue方法将元素添加到队列 enqueue(e); return true; &#125; finally &#123; lock.unlock(); &#125; &#125; //将数据添加到队列中的具体方法 private void enqueue(E x) &#123; // assert lock.getHoldCount() == 1; // assert items[putIndex] == null; final Object[] items = this.items; items[putIndex] = x; //通过循环数组实现的队列，当数组满了时下标就变成0了 if (++putIndex == items.length) putIndex = 0; count++; //激活因为notEmpty条件而阻塞的线程，比如调用take方法的线程 notEmpty.signal(); &#125; //将数据从队列中取出的方法 private E dequeue() &#123; // assert lock.getHoldCount() == 1; // assert items[takeIndex] != null; final Object[] items = this.items; @SuppressWarnings(&quot;unchecked&quot;) E x = (E) items[takeIndex]; //将对应的数组下标位置设置为null释放资源 items[takeIndex] = null; if (++takeIndex == items.length) takeIndex = 0; count--; if (itrs != null) itrs.elementDequeued(); //激活因为notFull条件而阻塞的线程，比如调用put方法的线程 notFull.signal(); return x; &#125; //put方法和offer方法不一样的地方在于，如果队列是满的话，它就会把调用put方法的线程阻塞，直到队列里有空间 public void put(E e) throws InterruptedException &#123; checkNotNull(e); final ReentrantLock lock = this.lock; //因为后面调用了条件变量的await()方法，而await()方法会在中断标志设置后抛出InterruptedException异常后退出， // 所以在加锁时候先看中断标志是不是被设置了，如果设置了直接抛出InterruptedException异常，就不用再去获取锁了 lock.lockInterruptibly(); try &#123; while (count == items.length) //如果队列满的话就阻塞等待，直到notFull的signal方法被调用，也就是队列里有空间了 notFull.await(); //队列里有空间了执行添加操作 enqueue(e); &#125; finally &#123; lock.unlock(); &#125; &#125; //poll方法用于从队列中取数据，不会阻塞当前线程 public E poll() &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; //如果队列为空的话会直接返回null，否则调用dequeue方法取数据 return (count == 0) ? null : dequeue(); &#125; finally &#123; lock.unlock(); &#125; &#125; //有等待时间的 poll 重载方法 public E poll(long timeout, TimeUnit unit) throws InterruptedException &#123; long nanos = unit.toNanos(timeout); final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try &#123; while (count == 0) &#123; if (nanos &lt;= 0) return null; nanos = notEmpty.awaitNanos(nanos); &#125; return dequeue(); &#125; finally &#123; lock.unlock(); &#125; &#125; //take方法也是用于取队列中的数据，但是和poll方法不同的是它有可能会阻塞当前的线程 public E take() throws InterruptedException &#123; final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try &#123; //当队列为空时，就会阻塞当前线程 while (count == 0) notEmpty.await(); //直到队列中有数据了，调用dequeue方法将数据返回 return dequeue(); &#125; finally &#123; lock.unlock(); &#125; &#125; //返回队首元素 public E peek() &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; return itemAt(takeIndex); // null when queue is empty &#125; finally &#123; lock.unlock(); &#125; &#125; //获取队列的元素个数，加了锁，所以结果是准确的 public int size() &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; return count; &#125; finally &#123; lock.unlock(); &#125; &#125; // 此外，还有一些其他方法 //返回队列剩余空间，还能加几个元素 public int remainingCapacity() &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; return items.length - count; &#125; finally &#123; lock.unlock(); &#125; &#125; // 判断队列中是否存在当前元素o public boolean contains(Object o)&#123;&#125; // 返回一个按正确顺序，包含队列中所有元素的数组 public Object[] toArray()&#123;&#125; // 自动清空队列中的所有元素 public void clear()&#123;&#125; // 移除队列中所有可用元素，并将他们加入到给定的 Collection 中 public int drainTo(Collection&lt;? super E&gt; c)&#123;&#125; // 返回此队列中按正确顺序进行迭代的，包含所有元素的迭代器 public Iterator&lt;E&gt; iterator()&#125; LinkedBlockingQueueLinkedBlockingQueue 是一个用单向链表实现的有界阻塞队列。此队列的默认和最大长度为 Integer.MAX_VALUE。此队列按照先进先出的原则对元素进行排序。 如果不是特殊业务，LinkedBlockingQueue 使用时，切记要定义容量 new LinkedBlockingQueue(capacity) ，防止过度膨胀。 源码解读 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290public class LinkedBlockingQueue&lt;E&gt; extends AbstractQueue&lt;E&gt; implements BlockingQueue&lt;E&gt;, java.io.Serializable &#123; private static final long serialVersionUID = -6903933977591709194L; // 基于链表实现，肯定要有结点类，典型的单链表结构 static class Node&lt;E&gt; &#123; E item; Node&lt;E&gt; next; Node(E x) &#123; item = x; &#125; &#125; //容量 private final int capacity; //当前队列元素数量 private final AtomicInteger count = new AtomicInteger(); // 头节点，不存数据 transient Node&lt;E&gt; head; // 尾节点，便于入队 private transient Node&lt;E&gt; last; // take锁，出队锁，只有take，poll方法会持有 private final ReentrantLock takeLock = new ReentrantLock(); // 出队等待条件 // 当队列无元素时，take锁会阻塞在notEmpty条件上，等待其它线程唤醒 private final Condition notEmpty = takeLock.newCondition(); // 入队锁，只有put，offer会持有 private final ReentrantLock putLock = new ReentrantLock(); // 入队等待条件 // 当队列满了时，put锁会会阻塞在notFull上，等待其它线程唤醒 private final Condition notFull = putLock.newCondition(); //同样提供三个构造器 public LinkedBlockingQueue(int capacity) &#123; if (capacity &lt;= 0) throw new IllegalArgumentException(); // 初始化head和last指针为空值节点 this.capacity = capacity; last = head = new Node&lt;E&gt;(null); &#125; public LinkedBlockingQueue() &#123; // 如果没传容量，就使用最大int值初始化其容量 this(Integer.MAX_VALUE); &#125; public LinkedBlockingQueue(Collection&lt;? extends E&gt; c) &#123;&#125; //入队 public void put(E e) throws InterruptedException &#123; // 不允许null元素 if (e == null) throw new NullPointerException(); //规定给当前put方法预留一个本地变量 int c = -1; // 新建一个节点 Node&lt;E&gt; node = new Node&lt;E&gt;(e); final ReentrantLock putLock = this.putLock; final AtomicInteger count = this.count; // 使用put锁加锁 putLock.lockInterruptibly(); try &#123; // 如果队列满了，就阻塞在notFull条件上 // 等待被其它线程唤醒 while (count.get() == capacity) &#123; notFull.await(); &#125; // 队列不满了，就入队 enqueue(node); // 队列长度加1 c = count.getAndIncrement(); // 如果现队列长度小于容量 // 就再唤醒一个阻塞在notFull条件上的线程 // 这里为啥要唤醒一下呢？ // 因为可能有很多线程阻塞在notFull这个条件上的 // 而取元素时只有取之前队列是满的才会唤醒notFull // 为什么队列满的才唤醒notFull呢？ // 因为唤醒是需要加putLock的，这是为了减少锁的次数 // 所以，这里索性在放完元素就检测一下，未满就唤醒其它notFull上的线程 // 说白了，这也是锁分离带来的代价 if (c + 1 &lt; capacity) notFull.signal(); &#125; finally &#123; // 释放锁 putLock.unlock(); &#125; // 如果原队列长度为0，现在加了一个元素后立即唤醒notEmpty条件 if (c == 0) signalNotEmpty(); &#125; private void signalNotEmpty() &#123; final ReentrantLock takeLock = this.takeLock; // 加take锁 takeLock.lock(); try &#123; // 唤醒notEmpty条件 notEmpty.signal(); &#125; finally &#123; takeLock.unlock(); &#125; &#125; private void signalNotFull() &#123; final ReentrantLock putLock = this.putLock; putLock.lock(); try &#123; notFull.signal(); &#125; finally &#123; putLock.unlock(); &#125; &#125; private void enqueue(Node&lt;E&gt; node) &#123; // 直接加到last后面 last = last.next = node; &#125; public boolean offer(E e) &#123; //用带过期时间的说明 &#125; public boolean offer(E e, long timeout, TimeUnit unit) throws InterruptedException &#123; if (e == null) throw new NullPointerException(); //转换为纳秒 long nanos = unit.toNanos(timeout); int c = -1; final ReentrantLock putLock = this.putLock; final AtomicInteger count = this.count; //获取入队锁，支持等待锁的过程中被中断 putLock.lockInterruptibly(); try &#123; //队列满了，再看看有没有超时 while (count.get() == capacity) &#123; if (nanos &lt;= 0) //等待时间超时 return false; //进行等待，awaitNanos(long nanos)是AQS中的方法 //在等待过程中，如果被唤醒或超时，则继续当前循环 //如果被中断，则抛出中断异常 nanos = notFull.awaitNanos(nanos); &#125; //进入队尾 enqueue(new Node&lt;E&gt;(e)); c = count.getAndIncrement(); //说明当前元素后面还能再插入一个 //就唤醒一个入队条件队列中阻塞的线程 if (c + 1 &lt; capacity) notFull.signal(); &#125; finally &#123; putLock.unlock(); &#125; //节点数量为0，说明队列是空的 if (c == 0) //唤醒一个出队条件队列阻塞的线程 signalNotEmpty(); return true; &#125; public E take() throws InterruptedException &#123; E x; int c = -1; final AtomicInteger count = this.count; final ReentrantLock takeLock = this.takeLock; takeLock.lockInterruptibly(); try &#123; // 如果队列无元素，则阻塞在notEmpty条件上 while (count.get() == 0) &#123; notEmpty.await(); &#125; // 否则，出队 x = dequeue(); // 获取出队前队列的长度 c = count.getAndDecrement(); // 如果取之前队列长度大于1，则唤醒notEmpty if (c &gt; 1) notEmpty.signal(); &#125; finally &#123; takeLock.unlock(); &#125; // 如果取之前队列长度等于容量 // 则唤醒notFull if (c == capacity) signalNotFull(); return x; &#125; private E dequeue() &#123; Node&lt;E&gt; h = head; Node&lt;E&gt; first = h.next; h.next = h; // help GC head = first; E x = first.item; first.item = null; return x; &#125; public E poll(long timeout, TimeUnit unit) throws InterruptedException &#123; E x = null; int c = -1; long nanos = unit.toNanos(timeout); final AtomicInteger count = this.count; final ReentrantLock takeLock = this.takeLock; takeLock.lockInterruptibly(); try &#123; while (count.get() == 0) &#123; //队列为空且已经超时，直接返回空 if (nanos &lt;= 0) return null; //等待过程中可能被唤醒，超时，中断 nanos = notEmpty.awaitNanos(nanos); &#125; //进行出队操作 x = dequeue(); c = count.getAndDecrement(); if (c &gt; 1) notEmpty.signal(); &#125; finally &#123; takeLock.unlock(); &#125; //如果出队前，队列是满的，则唤醒一个被take()阻塞的线程 if (c == capacity) signalNotFull(); return x; &#125; public E poll() &#123; // &#125; public E peek() &#123; if (count.get() == 0) return null; final ReentrantLock takeLock = this.takeLock; takeLock.lock(); try &#123; Node&lt;E&gt; first = head.next; if (first == null) return null; else return first.item; &#125; finally &#123; takeLock.unlock(); &#125; &#125; void unlink(Node&lt;E&gt; p, Node&lt;E&gt; trail) &#123; // assert isFullyLocked(); // p.next is not changed, to allow iterators that are // traversing p to maintain their weak-consistency guarantee. p.item = null; trail.next = p.next; if (last == p) last = trail; if (count.getAndDecrement() == capacity) notFull.signal(); &#125; public boolean remove(Object o) &#123; if (o == null) return false; fullyLock(); try &#123; for (Node&lt;E&gt; trail = head, p = trail.next; p != null; trail = p, p = p.next) &#123; if (o.equals(p.item)) &#123; unlink(p, trail); return true; &#125; &#125; return false; &#125; finally &#123; fullyUnlock(); &#125; &#125; public boolean contains(Object o) &#123; &#125; static final class LBQSpliterator&lt;E&gt; implements Spliterator&lt;E&gt; &#123; &#125;&#125; LinkedBlockingQueue 与 ArrayBlockingQueue对比 ArrayBlockingQueue 入队出队采用一把锁，导致入队出队相互阻塞，效率低下； LinkedBlockingQueue 入队出队采用两把锁，入队出队互不干扰，效率较高； 二者都是有界队列，如果长度相等且出队速度跟不上入队速度，都会导致大量线程阻塞； LinkedBlockingQueue 如果初始化不传入初始容量，则使用最大 int 值，如果出队速度跟不上入队速度，会导致队列特别长，占用大量内存； PriorityBlockingQueuePriorityBlockingQueue 是一个支持优先级的无界阻塞队列。(虽说是无界队列，但是由于资源耗尽的话，也会OutOfMemoryError，无法添加元素) 默认情况下元素采用自然顺序升序排列。也可以自定义类实现 compareTo() 方法来指定元素排序规则，或者初始化 PriorityBlockingQueue 时，指定构造参数 Comparator 来对元素进行排序。但需要注意的是不能保证同优先级元素的顺序。PriorityBlockingQueue 是基于最小二叉堆实现，使用基于 CAS 实现的自旋锁来控制队列的动态扩容，保证了扩容操作不会阻塞 take 操作的执行。 DelayQueueDelayQueue 是一个使用优先级队列实现的延迟无界阻塞队列。 队列使用 PriorityQueue 来实现。队列中的元素必须实现 Delayed 接口，在创建元素时可以指定多久才能从队列中获取当前元素。只有在延迟期满时才能从队列中提取元素。我们可以将 DelayQueue 运用在以下应用场景： 缓存系统的设计：可以用 DelayQueue 保存缓存元素的有效期，使用一个线程循环查询 DelayQueue，一旦能从 DelayQueue 中获取元素时，表示缓存有效期到了。 定时任务调度。使用 DelayQueue 保存当天将会执行的任务和执行时间，一旦从 DelayQueue 中获取到任务就开始执行，从比如 Timer 就是使用 DelayQueue 实现的。 SynchronousQueueSynchronousQueue 是一个不存储元素的阻塞队列，也即是单个元素的队列。 每一个 put 操作必须等待一个 take 操作，否则不能继续添加元素。SynchronousQueue 可以看成是一个传球手，负责把生产者线程处理的数据直接传递给消费者线程。队列本身并不存储任何元素，非常适合于传递性场景, 比如在一个线程中使用的数据，传递给另外一个线程使用，SynchronousQueue 的吞吐量高于 LinkedBlockingQueue 和 ArrayBlockingQueue。 Coding synchronousQueue 是一个没有数据缓冲的阻塞队列，生产者线程对其的插入操作 put() 必须等待消费者的移除操作 take()，反过来也一样。 对应 peek, contains, clear, isEmpty … 等方法其实是无效的。 但是 poll() 和 offer() 就不会阻塞，举例来说就是 offer 的时候如果有消费者在等待那么就会立马满足返回 true，如果没有就会返回 false，不会等待消费者到来。 12345678910111213141516171819202122232425262728293031323334353637383940public class SynchronousQueueDemo &#123; public static void main(String[] args) &#123; BlockingQueue&lt;String&gt; queue = new SynchronousQueue&lt;&gt;(); //System.out.println(queue.offer(&quot;aaa&quot;)); //false //System.out.println(queue.poll()); //null System.out.println(queue.add(&quot;bbb&quot;)); //IllegalStateException: Queue full new Thread(()-&gt;&#123; try &#123; System.out.println(&quot;Thread 1 put a&quot;); queue.put(&quot;a&quot;); System.out.println(&quot;Thread 1 put b&quot;); queue.put(&quot;b&quot;); System.out.println(&quot;Thread 1 put c&quot;); queue.put(&quot;c&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;).start(); new Thread(()-&gt;&#123; try &#123; TimeUnit.SECONDS.sleep(2); System.out.println(&quot;Thread 2 get:&quot;+queue.take()); TimeUnit.SECONDS.sleep(2); System.out.println(&quot;Thread 2 get:&quot;+queue.take()); TimeUnit.SECONDS.sleep(2); System.out.println(&quot;Thread 2 get:&quot;+queue.take()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;).start(); &#125;&#125; 123456Thread 1 put aThread 2 get:aThread 1 put bThread 2 get:bThread 1 put cThread 2 get:c 源码解读 不像ArrayBlockingQueue、LinkedBlockingDeque之类的阻塞队列依赖AQS实现并发操作，SynchronousQueue直接使用CAS实现线程的安全访问。 synchronousQueue 提供了两个构造器（公平与否），内部是通过 Transferer 来实现的，具体分为两个Transferer，分别是 TransferStack 和 TransferQueue。 TransferStack：非公平竞争模式使用的数据结构是后进先出栈(LIFO Stack) TransferQueue：公平竞争模式则使用先进先出队列（FIFO Queue） 性能上两者是相当的，一般情况下，FIFO 通常可以支持更大的吞吐量，但 LIFO 可以更大程度的保持线程的本地化。 123456789private transient volatile Transferer&lt;E&gt; transferer;public SynchronousQueue() &#123; this(false);&#125;public SynchronousQueue(boolean fair) &#123; transferer = fair ? new TransferQueue&lt;E&gt;() : new TransferStack&lt;E&gt;();&#125; 分析 TransferQueue 的实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758//构造函数中会初始化一个出队的节点，并且首尾都指向这个节点TransferQueue() &#123; QNode h = new QNode(null, false); // initialize to dummy node. head = h; tail = h;&#125;//队列节点,static final class QNode &#123; volatile QNode next; // next node in queue volatile Object item; // CAS&#x27;ed to or from null volatile Thread waiter; // to control park/unpark final boolean isData; QNode(Object item, boolean isData) &#123; this.item = item; this.isData = isData; &#125; // 设置next和item的值，用于进行并发更新, cas 无锁操作 boolean casNext(QNode cmp, QNode val) &#123; return next == cmp &amp;&amp; UNSAFE.compareAndSwapObject(this, nextOffset, cmp, val); &#125; boolean casItem(Object cmp, Object val) &#123; return item == cmp &amp;&amp; UNSAFE.compareAndSwapObject(this, itemOffset, cmp, val); &#125; void tryCancel(Object cmp) &#123; UNSAFE.compareAndSwapObject(this, itemOffset, cmp, this); &#125; boolean isCancelled() &#123; return item == this; &#125; boolean isOffList() &#123; return next == this; &#125; // Unsafe mechanics private static final sun.misc.Unsafe UNSAFE; private static final long itemOffset; private static final long nextOffset; static &#123; try &#123; UNSAFE = sun.misc.Unsafe.getUnsafe(); Class&lt;?&gt; k = QNode.class; itemOffset = UNSAFE.objectFieldOffset (k.getDeclaredField(&quot;item&quot;)); nextOffset = UNSAFE.objectFieldOffset (k.getDeclaredField(&quot;next&quot;)); &#125; catch (Exception e) &#123; throw new Error(e); &#125; &#125;&#125; 从 put() 方法和 take() 方法可以看出最终调用的都是 TransferQueue 的 transfer() 方法。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283public void put(E e) throws InterruptedException &#123; if (e == null) throw new NullPointerException(); if (transferer.transfer(e, false, 0) == null) &#123; Thread.interrupted(); throw new InterruptedException(); &#125;&#125;public E take() throws InterruptedException &#123; E e = transferer.transfer(null, false, 0); if (e != null) return e; Thread.interrupted(); throw new InterruptedException();&#125;//transfer方法用于提交数据或者是获取数据E transfer(E e, boolean timed, long nanos) &#123; QNode s = null; // constructed/reused as needed //如果e不为null，就说明是添加数据的入队操作 boolean isData = (e != null); for (;;) &#123; QNode t = tail; QNode h = head; if (t == null || h == null) // saw uninitialized value continue; // spin //如果当前操作和 tail 节点的操作是一样的；或者头尾相同（表明队列中啥都没有）。 if (h == t || t.isData == isData) &#123; // empty or same-mode QNode tn = t.next; // 如果 t 和 tail 不一样，说明，tail 被其他的线程改了，重来 if (t != tail) // inconsistent read continue; // 如果 tail 的 next 不是空。就需要将 next 追加到 tail 后面了 if (tn != null) &#123; // lagging tail // 使用 CAS 将 tail.next 变成 tail, advanceTail(t, tn); continue; &#125; // 时间到了，不等待，返回 null，插入失败，获取也是失败的 if (timed &amp;&amp; nanos &lt;= 0) // can&#x27;t wait return null; if (s == null) s = new QNode(e, isData); if (!t.casNext(null, s)) // failed to link in continue; advanceTail(t, s); // swing tail and wait Object x = awaitFulfill(s, e, timed, nanos); if (x == s) &#123; // wait was cancelled clean(t, s); return null; &#125; if (!s.isOffList()) &#123; // not already unlinked advanceHead(t, s); // unlink if head if (x != null) // and forget fields s.item = s; s.waiter = null; &#125; return (x != null) ? (E)x : e; &#125; else &#123; // complementary-mode QNode m = h.next; // node to fulfill if (t != tail || m == null || h != head) continue; // inconsistent read Object x = m.item; if (isData == (x != null) || // m already fulfilled x == m || // m cancelled !m.casItem(x, e)) &#123; // lost CAS advanceHead(h, m); // dequeue and retry continue; &#125; advanceHead(h, m); // successfully fulfilled LockSupport.unpark(m.waiter); return (x != null) ? (E)x : e; &#125; &#125;&#125; LinkedTransferQueueLinkedTransferQueue 是一个由链表结构组成的无界阻塞 TransferQueue 队列。 LinkedTransferQueue采用一种预占模式。意思就是消费者线程取元素时，如果队列不为空，则直接取走数据，若队列为空，那就生成一个节点（节点元素为null）入队，然后消费者线程被等待在这个节点上，后面生产者线程入队时发现有一个元素为null的节点，生产者线程就不入队了，直接就将元素填充到该节点，并唤醒该节点等待的线程，被唤醒的消费者线程取走元素，从调用的方法返回。我们称这种节点操作为“匹配”方式。 队列实现了 TransferQueue 接口重写了 tryTransfer 和 transfer 方法，这组方法和 SynchronousQueue 公平模式的队列类似，具有匹配的功能 LinkedBlockingDequeLinkedBlockingDeque 是一个由链表结构组成的双向阻塞队列。 所谓双向队列指的你可以从队列的两端插入和移出元素。双端队列因为多了一个操作队列的入口，在多线程同时入队时，也就减少了一半的竞争。相比其他的阻塞队列，LinkedBlockingDeque 多了 addFirst，addLast，offerFirst，offerLast，peekFirst，peekLast 等方法，以 First 单词结尾的方法，表示插入，获取（peek）或移除双端队列的第一个元素。以 Last 单词结尾的方法，表示插入，获取或移除双端队列的最后一个元素。另外插入方法 add 等同于 addLast，移除方法 remove 等效于 removeFirst。 在初始化 LinkedBlockingDeque 时可以设置容量防止其过渡膨胀，默认容量也是 Integer.MAX_VALUE。另外双向阻塞队列可以运用在“工作窃取”模式中。 阻塞队列使用场景我们常用的生产者消费者模式就可以基于阻塞队列实现； 线程池中活跃线程数达到 corePoolSize 时，线程池将会将后续的 task 提交到 BlockingQueue 中； 生产者消费者模式JDK API文档的 BlockingQueue 给出了一个典型的应用 面试题：一个初始值为 0 的变量，两个线程对齐交替操作，一个+1，一个-1，5 轮 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public class ProdCounsume_TraditionDemo &#123; public static void main(String[] args) &#123; ShareData shareData = new ShareData(); new Thread(() -&gt; &#123; for (int i = 0; i &lt;= 5; i++) &#123; shareData.increment(); &#125; &#125;, &quot;T1&quot;).start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt;= 5; i++) &#123; shareData.decrement(); &#125; &#125;, &quot;T1&quot;).start(); &#125;&#125;//线程操作资源类class ShareData &#123; private int num = 0; private Lock lock = new ReentrantLock(); private Condition condition = lock.newCondition(); public void increment() &#123; lock.lock(); try &#123; while (num != 0) &#123; //等待，不能生产 condition.await(); &#125; //干活 num++; System.out.println(Thread.currentThread().getName() + &quot;\\t&quot; + num); //唤醒 condition.signal(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void decrement() &#123; lock.lock(); try &#123; while (num == 0) &#123; //等待，不能生产 condition.await(); &#125; //干活 num--; System.out.println(Thread.currentThread().getName() + &quot;\\t&quot; + num); //唤醒 condition.signal(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125;&#125; 线程池线程池的核心方法 ThreadPoolExecutor，用 BlockingQueue 存放任务的阻塞队列，被提交但尚未被执行的任务 1234567public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) 线程池在内部实际也是构建了一个生产者消费者模型，将线程和任务两者解耦，并不直接关联，从而良好的缓冲任务，复用线程。 不同的线程池实现用的是不同的阻塞队列，newFixedThreadPool 和 newSingleThreadExecutor 用的是LinkedBlockingQueue，newCachedThreadPool 用的是 SynchronousQueue。 参考： Java 7 种阻塞队列详解 - 腾讯云开发者社区-腾讯云 (tencent.com)","categories":[{"name":"技术","slug":"技术","permalink":"https://xssdpgy.github.io/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"队列","slug":"队列","permalink":"https://xssdpgy.github.io/tags/%E9%98%9F%E5%88%97/"}]},{"title":"TimeLimiter接口超时中断实现分析","slug":"TimeLimiter接口超时中断实现分析","date":"2023-01-31T10:04:32.000Z","updated":"2023-02-07T16:26:00.599Z","comments":true,"path":"2023/01/31/TimeLimiter接口超时中断实现分析/","link":"","permalink":"https://xssdpgy.github.io/2023/01/31/TimeLimiter%E6%8E%A5%E5%8F%A3%E8%B6%85%E6%97%B6%E4%B8%AD%E6%96%AD%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90/","excerpt":"0.背景","text":"0.背景 前段时间，其他组同事找到我，说让我协助看一个事务未回滚问题，刚开始以为是事务隔离设置方面的原因，结果原因很简单，因为要给接口添加超时中断的功能，他根据网上文章使用java.util.concurrent.Future#get(long, java.util.concurrent.TimeUnit)来实现超时中断功能，结果超时功能实现了，但是超时抛出异常后接口事务没有回滚，我看了下代码，主要原因是方法中使用的是声明式事务，对于新引入的异步任务来说事务管理粒度太粗糙，且捕获超时异常后，任务未手动取消。下意识准备调整为编程式事务来使事务管理细化，结果刚要下手时想到不是有现成的轮子嘛，下面就该轮子——guava的TimeLimiter进行简要介绍。 1.使用场景描述网上找了段话描述接口超时中断的需求场景： 如果调用方法超过1秒，就应该停止调用，不要一直阻塞下去，防止把本身的服务资源搞挂。 在不可预知可能出现死锁&#x2F;死循环的代码，要加上时间的阀值，避免阻塞。 2.TimeLimiter实现接口超时中断guava工具包里面包含了超时的控制方法。即TimeLimiter 接口，其有两个实现类。 FakeTimeLimiter, 常用于debug调试时，限制时间超时调试。 SimpleTimeLimiter 常用于正式方法中，调用方法超时，即抛出异常。 我们在实际开发中也是使用SimpleTimeLimiter来实现超时控制功能，其有两种实现模式：代理模式，回调模式。实现很简单，看代码即可了解。 3.SimpleTimeLimiter 的源码实现3.1 代理模式 com.google.common.util.concurrent.SimpleTimeLimiter#newProxy 12345678910111213141516171819202122232425262728293031323334@Override public &lt;T&gt; T newProxy(final T target, Class&lt;T&gt; interfaceType, final long timeoutDuration, final TimeUnit timeoutUnit) &#123; checkNotNull(target); checkNotNull(interfaceType); checkNotNull(timeoutUnit); checkArgument(timeoutDuration &gt; 0, &quot;bad timeout: %s&quot;, timeoutDuration); checkArgument(interfaceType.isInterface(), &quot;interfaceType must be an interface type&quot;); final Set&lt;Method&gt; interruptibleMethods = findInterruptibleMethods(interfaceType); InvocationHandler handler = new InvocationHandler() &#123; @Override public Object invoke(Object obj, final Method method, final Object[] args) throws Throwable &#123; Callable&lt;Object&gt; callable = new Callable&lt;Object&gt;() &#123; @Override public Object call() throws Exception &#123; try &#123; return method.invoke(target, args); &#125; catch (InvocationTargetException e) &#123; throwCause(e, false); throw new AssertionError(&quot;can&#x27;t get here&quot;); &#125; &#125; &#125;; return callWithTimeout(callable, timeoutDuration, timeoutUnit, interruptibleMethods.contains(method)); &#125; &#125;; return newProxy(interfaceType, handler); &#125; 3.2 回调模式 com.google.common.util.concurrent.SimpleTimeLimiter#callWithTimeout 1234567891011121314151617181920212223242526272829@Override public &lt;T&gt; T callWithTimeout(Callable&lt;T&gt; callable, long timeoutDuration, TimeUnit timeoutUnit, boolean amInterruptible) throws Exception &#123; checkNotNull(callable); checkNotNull(timeoutUnit); checkArgument(timeoutDuration &gt; 0, &quot;timeout must be positive: %s&quot;, timeoutDuration); Future&lt;T&gt; future = executor.submit(callable); try &#123; if (amInterruptible) &#123; try &#123; //实际也是通过Future#get(long, java.util.concurrent.TimeUnit)实现超时 return future.get(timeoutDuration, timeoutUnit); &#125; catch (InterruptedException e) &#123; future.cancel(true); throw e; &#125; &#125; else &#123; return Uninterruptibles.getUninterruptibly(future, timeoutDuration, timeoutUnit); &#125; &#125; catch (ExecutionException e) &#123; throw throwCause(e, true); &#125; catch (TimeoutException e) &#123; //超时异常取消任务并抛出UncheckedTimeoutException异常 future.cancel(true); throw new UncheckedTimeoutException(e); &#125; &#125; 代理模式主要针对类，回调模式可以针对某部分代码，可以看到代理模式也是基于回调模式方法做了层代码封装，超时控制的底层实现还是在SimpleTimeLimiter#callWithTimeout，其基于Future#get(long, java.util.concurrent.TimeUnit)实现超时，因此SimpleTimeLimiter本质上也是使用了JDK中的Future对象实现了超时中断控制。 结合代理模式调用超时链路即很清晰的展示超时中断控制实现，此时接口仍旧使用声明式事务，超时后事务正常回滚。","categories":[{"name":"技术","slug":"技术","permalink":"https://xssdpgy.github.io/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"timeLimit","slug":"timeLimit","permalink":"https://xssdpgy.github.io/tags/timeLimit/"},{"name":"源码分析","slug":"源码分析","permalink":"https://xssdpgy.github.io/tags/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"}]},{"title":"Seata客户端集成及AT&TCC事务模式演示","slug":"Seata客户端集成及AT&TCC事务模式演示","date":"2023-01-30T02:54:43.000Z","updated":"2023-02-07T16:26:00.598Z","comments":true,"path":"2023/01/30/Seata客户端集成及AT&TCC事务模式演示/","link":"","permalink":"https://xssdpgy.github.io/2023/01/30/Seata%E5%AE%A2%E6%88%B7%E7%AB%AF%E9%9B%86%E6%88%90%E5%8F%8AAT&TCC%E4%BA%8B%E5%8A%A1%E6%A8%A1%E5%BC%8F%E6%BC%94%E7%A4%BA/","excerpt":"0.前言简要说下背景，当前使用seata是基于官方1.5.2版本开发的，所以集成过程可供1.5.2及之后版本的使用者参考，为区别于官方版本，内部版本号设置为1.5.2.2。设计demo演示全局事务，执行流程如下。 demo业务流程设计","text":"0.前言简要说下背景，当前使用seata是基于官方1.5.2版本开发的，所以集成过程可供1.5.2及之后版本的使用者参考，为区别于官方版本，内部版本号设置为1.5.2.2。设计demo演示全局事务，执行流程如下。 demo业务流程设计 1.Seata客户端集成1.1 引入依赖12345678910111213141516171819202122&lt;!-- springboot项目，引入seata-spring-boot-starter依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;io.seata&lt;/groupId&gt; &lt;artifactId&gt;seata-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.5.2.2&lt;/version&gt;&lt;/dependency&gt; &lt;!-- 为保证微服务间xid透传，引入此依赖（注意，需排除引入其他版本seata依赖的影响） --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-seata&lt;/artifactId&gt; &lt;version&gt;1.5.1.RELEASE&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;io.seata&lt;/groupId&gt; &lt;artifactId&gt;seata-spring-boot-starter&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;io.seata&lt;/groupId&gt; &lt;artifactId&gt;seata-all&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; 项目seata相关依赖版本统一如下： 1.2 配置以fundpay服务为例，配置文件中新增配置，主要为seata客户端服务配置注册中心及vgroupMapping设置。 之后可启动seata-server服务端，再启动fundpay客户端服务查看是否正常启动及注册。如下日志即为客户端服务启动成功并成功注册到seata-server服务端时，seata-server服务端的输出（打印前两行日志即可视为成功）。 其他微服务按此方式集成注册。 集成成功后，服务间拓扑如下 2.AT事务模式开发配置AT事务模式代码侵入性低，可以基本保持之前的代码结构，一般情况下，只需要在入口业务方法实现上添加@GlobalTransactional注解即可。 原先接口主要代码实现如下： 在com.boss.fundpay.service.impl.PayServiceImpl#pay业务实现方法上添加全局事务注解。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152 @Override @GlobalTransactional(rollbackFor = Exception.class,timeoutMills = 60000) public ApiResultDTO pay(PayInfoDto payInfo) throws Exception &#123; //1.记录当前操作 PayLog payLog = new PayLog(); BeanUtil.copyProperties(payInfo,payLog); payLog.setBgtId(IdUtil.simpleUUID()); String curDate = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;).format(new Date()); payLog.setPayTime(curDate); payLogDao.insertPayLog(payLog); String orderNo = payInfo.getOrderNo(); String merchantId = payInfo.getMerchantId(); //2.调用流程审核 RiskAuditReq riskAuditReq = new RiskAuditReq(); riskAuditReq.setMerchantId(merchantId); riskAuditReq.setOrderNo(orderNo); ApiResultDTO workflowResult = workflowFeignApi.riskAudit(riskAuditReq); if(Objects.isNull(workflowResult)||ApiResultDTO.hasError(workflowResult) ||(!PASS.equals(workflowResult.getResult())))&#123; throw new Exception(); &#125; //获取商户余额 MerchantBanlanceRecord record = merchantBalanceRecordDao.selectBalanceRecordById(merchantId); BigDecimal balance = record.getTotalBalance().add(payInfo.getTotalAmount()); //3.调用记账 IncreaseBillReq increaseBillReq = new IncreaseBillReq(); increaseBillReq.setOrderNo(orderNo); increaseBillReq.setAccountAmount(balance); increaseBillReq.setMerchantId(merchantId); ApiResultDTO accountResult = accountFeignApi.increaseBill(increaseBillReq); if(Objects.isNull(accountResult)||ApiResultDTO.hasError(accountResult))&#123; throw new Exception(); &#125; //4.回写本地表 record.setPayType(payLog.getPayType()); record.setPayTime(payLog.getPayTime()); record.setOrderNo(payLog.getOrderNo()); record.setTotalAmount(payLog.getTotalAmount()); record.setTotalBalance(balance); merchantBalanceRecordDao.updateBalanceRecord(record);//创造异常触发回滚// int a = RandomUtil.randomInt(10);// if(a==5)&#123;// int b=1/0;// &#125; //完成 return ApiResultDTO.success(&quot;成功了 order=&quot;+orderNo+&quot; ,merchantId=&quot;+merchantId+&quot; ,balance=&quot;+balance); &#125; 在AT模式下，workflow服务和account服务的代码逻辑不需要调整。 3.TCC事务模式开发配置3.1 两阶段提交回顾回顾下官网描述，一个分布式的全局事务，整体是 两阶段提交 的模型。全局事务是由若干分支事务组成的，分支事务要满足 两阶段提交 的模型要求，即需要每个分支事务都具备自己的： 一阶段 prepare 行为 二阶段 commit 或 rollback 行为 根据两阶段行为模式的不同，将分支事务划分为 Automatic (Branch) Transaction Mode 和 TCC (Branch) Transaction Mode。 AT 模式基于 支持本地 ACID 事务 的 关系型数据库： 一阶段 prepare 行为：在本地事务中，一并提交业务数据更新和相应回滚日志记录。 二阶段 commit 行为：马上成功结束，自动 异步批量清理回滚日志。 二阶段 rollback 行为：通过回滚日志，自动 生成补偿操作，完成数据回滚。 相应的，TCC 模式，不依赖于底层数据资源的事务支持： 一阶段 prepare 行为：调用 自定义 的 prepare 逻辑。 二阶段 commit 行为：调用 自定义 的 commit 逻辑。 二阶段 rollback 行为：调用 自定义 的 rollback 逻辑。 总之，TCC事务模式需要对代码逻辑进行重新设计，主要是需要按照 两阶段提交 的模型要求，开发自定义的分支事务，将其纳入全局事务的管理中。 3.2 接口实现方法拆分如下面示例，pay方法需要在接口层拆分出prepare、confirm、rollback三个子方法，将对应的代码逻辑拆分到对应子方法实现中，其他服务也按此模型思想进行代码拆分。 fundpay实现拆分示例（仅拆分fundpay自身的业务DB操作，涉及远程调用的不需考虑，详情见下节） workflow实现拆分示例 业务实现的粒度拆分需要开发人员合理把控 account服务拆分基本相同，这里省略。 3.3 入口业务方法独立上节是将各个微服务的业务实现代码按照 二阶段提交 的思想进行了拆分，回到业务入口方法这里，不可能也不应该在fundpay服务拆分后的二阶段方法中执行远程调用，所以独立出单独的入口方法Action，执行整体的业务逻辑（含服务间远程调用），将各分支事务纳入全局事务的控制中。","categories":[{"name":"技术","slug":"技术","permalink":"https://xssdpgy.github.io/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"seata","slug":"seata","permalink":"https://xssdpgy.github.io/tags/seata/"},{"name":"分布式事务","slug":"分布式事务","permalink":"https://xssdpgy.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"}]},{"title":"Seata源码结构及事务模式介绍","slug":"Seata源码结构及事务模式介绍","date":"2023-01-30T02:06:29.000Z","updated":"2023-02-07T16:26:00.599Z","comments":true,"path":"2023/01/30/Seata源码结构及事务模式介绍/","link":"","permalink":"https://xssdpgy.github.io/2023/01/30/Seata%E6%BA%90%E7%A0%81%E7%BB%93%E6%9E%84%E5%8F%8A%E4%BA%8B%E5%8A%A1%E6%A8%A1%E5%BC%8F%E4%BB%8B%E7%BB%8D/","excerpt":"1.Seata是什么Seata 是一款开源的分布式事务解决方案，致力于提供高性能和简单易用的分布式事务服务。Seata 将为用户提供了 AT、TCC、SAGA 和 XA 事务模式，为用户打造一站式的分布式解决方案。 2.Seata体系中的三个组件","text":"1.Seata是什么Seata 是一款开源的分布式事务解决方案，致力于提供高性能和简单易用的分布式事务服务。Seata 将为用户提供了 AT、TCC、SAGA 和 XA 事务模式，为用户打造一站式的分布式解决方案。 2.Seata体系中的三个组件 2.1 三个组件Seata 内部定义了 3个模块来处理全局事务和分支事务的关系和处理过程，这三个组件分别是： TC (Transaction Coordinator) - 事务协调者：维护全局和分支事务的状态，驱动全局事务提交或回滚。 TM (Transaction Manager) - 事务管理器：定义全局事务的范围：开始全局事务、提交或回滚全局事务。 RM (Resource Manager) - 资源管理器：管理分支事务处理的资源，与TC交谈以注册分支事务和报告分支事务的状态，并驱动分支事务提交或回滚。 2.2 执行步骤整个全局事务的执行步骤如下： TM 向 TC 申请开启一个全局事务，TC 创建全局事务后返回全局唯一的 XID，XID 会在全局事务的上下文中传播； RM 向 TC 注册分支事务，该分支事务归属于拥有相同 XID 的全局事务； TM 向 TC 发起全局提交或回滚； TC 调度 XID 下的分支事务完成提交或者回滚。 3.Seata源码体系整体结构 源码基本同官方1.5.2-1.6.1版本。 4.Seata事务模式介绍4.1 Seata AT 模式 两阶段提交协议的演变： 一阶段：业务数据和回滚日志记录在同一个本地事务中提交，释放本地锁和连接资源。 二阶段： 提交异步化，非常快速地完成。 回滚通过一阶段的回滚日志进行反向补偿。 4.2 Seata TCC 模式整体是两阶段提交的模型。 全局事务是由若干分支事务组成的，分支事务要满足两阶段提交的模型要求，即需要每个分支事务都具备自己的： 一阶段 prepare 行为 二阶段 commit 或 rollback 行为 4.3 Seata Saga 模式目前Seata提供的Saga模式是基于状态机引擎来实现的，机制是： 通过状态图来定义服务调用的流程并生成 json 状态语言定义文件。 状态图中一个节点可以是调用一个服务，节点可以配置它的补偿节点。 状态图 json 由状态机引擎驱动执行，当出现异常时状态引擎反向执行已成功节点对应的补偿节点将事务回滚。 可以实现服务编排需求，支持单项选择、并发、子流程、参数转换、参数映射、服务执行状态判断、异常捕获等功能。 4.4 Seata XA 模式利用事务资源（数据库、消息服务等）对 XA 协议的支持，以 XA 协议的机制来管理分支事务的一种 事务模式。 执行阶段： 可回滚：业务 SQL 操作放在 XA 分支中进行，由资源对 XA 协议的支持来保证 可回滚。 持久化：XA 分支完成后，执行 XA prepare，同样，由资源对 XA 协议的支持来保证持久化（即，之后任何意外都不会造成无法回滚的情况）。 完成阶段： 分支提交：执行 XA 分支的 commit 分支回滚：执行 XA 分支的 rollback","categories":[{"name":"技术","slug":"技术","permalink":"https://xssdpgy.github.io/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"seata","slug":"seata","permalink":"https://xssdpgy.github.io/tags/seata/"},{"name":"分布式事务","slug":"分布式事务","permalink":"https://xssdpgy.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"}]},{"title":"Seata AT模式源码解析","slug":"Seata-AT模式源码解析","date":"2022-11-27T10:00:36.000Z","updated":"2023-02-07T16:26:00.598Z","comments":true,"path":"2022/11/27/Seata-AT模式源码解析/","link":"","permalink":"https://xssdpgy.github.io/2022/11/27/Seata-AT%E6%A8%A1%E5%BC%8F%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/","excerpt":"1.AT模式的特性1.1 前提AT模式生效需要两个前提： 基于支持本地ACID事务的关系型数据库 Java应用，通过JDBC访问数据库 1.2 整体机制","text":"1.AT模式的特性1.1 前提AT模式生效需要两个前提： 基于支持本地ACID事务的关系型数据库 Java应用，通过JDBC访问数据库 1.2 整体机制两阶段提交协议的演变： 一阶段：业务数据和回滚日志记录在同一个本地事务中提交，释放本地锁和连接资源。 二阶段： 提交异步化，非常快速地完成。 回滚通过一阶段的回滚日志进行反向补偿。 1.3 写隔离 一阶段本地事务提交前，需要确保先拿到 全局锁 。 拿不到 全局锁 ，不能提交本地事务。 拿 全局锁 的尝试被限制在一定范围内，超出范围将放弃，并回滚本地事务，释放本地锁。 1.4 读隔离在数据库本地事务隔离级别 读已提交（Read Committed） 或以上的基础上，Seata（AT 模式）的默认全局隔离级别是 读未提交（Read Uncommitted） 。 如果应用在特定场景下，必需要求全局的 读已提交 ，目前 Seata 的方式是通过 SELECT FOR UPDATE 语句的代理。 2. 源码梳理 源码版本 1.5.2 2.1 TM及RM初始化TM和RM（Client端）由业务系统集成，AT模式下，在处理TC（Server端）发过来的请求时，这俩角色发挥着主要作用。所以从GlobalTransactionScanner中两者的初始化开始说起。 启动的时候，会调用GlobalTransactionScanner#initClient()方法，在initClient()中初始化TM和RM。 TM和RM初始化，主要是注册各种处理器，最终构造一个处理器映射表： RM注册处理器逻辑相同，这里不多展示。 需要关注的是RM初始化时，在注册各种处理器之前还有两个操作： RM初始化时，先是设置了 ResourceManager 和 TransactionMessageHandler，之后也是如TM初始化一样注册各种处理器，有区别的是注册处理器流程中，RM是注册完三个特有的客户端之后，才执行与TM类似的注册流程。 真正处理请求的还是靠调用各个处理器中的handler.onRequest()方法，于是问题的关键就很明显了，就在于handler。 2.2 ResourceManager在了解ResourceManager之前，需要先了解下ResourceManagerInbound和ResourceManagerOutbound。 ResourceManagerInbound是处理接收到TC的请求的，是TC向RM发请求 ResourceManagerOutbound是处理流出的消息的，是RM向TC发请求 ResourceManager继承了二者，所以既负责向TC发请求，又负责接收从TC来的请求。 DefaultResourceManager.get()得到的是一个单例DefaultResourceManager，创建DefaultResourceManager的时候会构建一个分支类型为ResourceManager的一个Map。","categories":[{"name":"技术","slug":"技术","permalink":"https://xssdpgy.github.io/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"seata","slug":"seata","permalink":"https://xssdpgy.github.io/tags/seata/"},{"name":"分布式事务","slug":"分布式事务","permalink":"https://xssdpgy.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"}]},{"title":"代理模式——JDK动态代理与CGLib原理及对比分析","slug":"代理模式——JDK动态代理与CGLib原理及对比分析","date":"2022-10-12T16:05:28.000Z","updated":"2022-10-25T15:10:58.613Z","comments":true,"path":"2022/10/13/代理模式——JDK动态代理与CGLib原理及对比分析/","link":"","permalink":"https://xssdpgy.github.io/2022/10/13/%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F%E2%80%94%E2%80%94JDK%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E4%B8%8ECGLib%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AF%B9%E6%AF%94%E5%88%86%E6%9E%90/","excerpt":"1.前言首先回顾下代理模式（Proxy Pattern）的定义：代理模式指为其他对象提供一种代理，以控制这个对象的访问，属于结构型设计模式。其适用于在某些情况下，一个对象不适合或者不能直接引用另一个对象，而代理对象可以在客户端于目标对象之间起到中介的作用。","text":"1.前言首先回顾下代理模式（Proxy Pattern）的定义：代理模式指为其他对象提供一种代理，以控制这个对象的访问，属于结构型设计模式。其适用于在某些情况下，一个对象不适合或者不能直接引用另一个对象，而代理对象可以在客户端于目标对象之间起到中介的作用。 代理模式主要分为静态代理和动态代理两种方式，静态代理需要手动创建代理类，代理的目标对象是固定的；动态代理使用反射机制，代理的目标对象是活动的，不需要创建代理类即可给不同的目标随时创建代理。本篇重点探究动态代理的实现。 2.JDK动态代理JDK动态代理采用字节重组，重新生成对象来替代原始对象，以达到动态代理的目的。JDK动态代理生成对象的步骤如下： 获取被代理对象的引用，并且获取它的所有接口，反射获取。 JDK动态代理类重新生成一个新的类，同时新的类要实现被代理类实现的所有接口。 动态生成Java代码，新加的业务逻辑方法由一定的逻辑代码调用（在代码中体现）。 编译新生成的Java代码.class文件。 重新加载到JVM中运行。 2.1 JDK动态代理实现及原理源码解析实现一个JDK动态代理，方式为实现java.lang.reflect.InvocationHandler接口，并使用java.lang.reflect.Proxy.newProxyInstance()方法生成代理对象。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859/*** 要代理的接口*/public interface IPerson &#123; void learn();&#125;/*** 真实调用类*/public class Zhangsan implements IPerson &#123; public void learn() &#123; System.out.println(&quot;==张三学习中间件==&quot;); &#125;&#125;/*** JDK代理类生成*/import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;public class JdkInvocationHandler implements InvocationHandler &#123; private IPerson target; public IPerson getInstance(IPerson target)&#123; this.target = target; Class&lt;?&gt; clazz = target.getClass(); return (IPerson) Proxy.newProxyInstance(clazz.getClassLoader(),clazz.getInterfaces(),this); &#125; public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; before(); Object result = method.invoke(this.target,args); after(); return result; &#125; private void before() &#123; System.out.println(&quot;事前做好计划&quot;); &#125; private void after() &#123; System.out.println(&quot;事后回顾梳理&quot;); &#125;&#125;/*** 测试*/public class TestProxy &#123; public static void main(String[] args) &#123; try &#123; //把生成的字节码保存到本地磁盘,动态生成的类会保存在工程根目录下的 com/sun/proxy 目录里面 System.setProperty(&quot;sun.misc.ProxyGenerator.saveGeneratedFiles&quot;,&quot;true&quot;); IPerson obj = (IPerson) new JdkInvocationHandler().getInstance(new Zhangsan()); obj.learn(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 看下 Proxy.newProxyInstance 里面究竟发生了什么？ 结合流程图，在生成字节码的那个地方，也就是 ProxyGenerator.generateProxyClass() 方法里面，通过代码可以看到（自行查阅，篇幅原因，这里不贴代码），里面是用参数 saveGeneratedFiles 来控制是否把生成的字节码保存到本地磁盘。代码中已经设置保存到本地，现在找到刚才生成的 $Proxy0.class，反编译打开如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import com.zang.jdkproxy.IPerson;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;import java.lang.reflect.UndeclaredThrowableException;public final class $Proxy0 extends Proxy implements IPerson &#123; private static Method m1; private static Method m3; private static Method m2; private static Method m0; public $Proxy0(InvocationHandler var1) throws &#123; super(var1); &#125; public final boolean equals(Object var1) throws &#123; try &#123; return (Boolean)super.h.invoke(this, m1, new Object[]&#123;var1&#125;); &#125; catch (RuntimeException | Error var3) &#123; throw var3; &#125; catch (Throwable var4) &#123; throw new UndeclaredThrowableException(var4); &#125; &#125; public final void learn() throws &#123; try &#123; // super.h 对应的是父类的h变量，也就是Proxy.newProxyInstance方法中的InvocationHandler参数 // 所以这里实际上就是使用了我们自己写的InvocationHandler实现类的invoke方法 super.h.invoke(this, m3, (Object[])null); &#125; catch (RuntimeException | Error var2) &#123; throw var2; &#125; catch (Throwable var3) &#123; throw new UndeclaredThrowableException(var3); &#125; &#125; public final String toString() throws &#123; try &#123; return (String)super.h.invoke(this, m2, (Object[])null); &#125; catch (RuntimeException | Error var2) &#123; throw var2; &#125; catch (Throwable var3) &#123; throw new UndeclaredThrowableException(var3); &#125; &#125; public final int hashCode() throws &#123; try &#123; return (Integer)super.h.invoke(this, m0, (Object[])null); &#125; catch (RuntimeException | Error var2) &#123; throw var2; &#125; catch (Throwable var3) &#123; throw new UndeclaredThrowableException(var3); &#125; &#125; static &#123; try &#123; m1 = Class.forName(&quot;java.lang.Object&quot;).getMethod(&quot;equals&quot;, Class.forName(&quot;java.lang.Object&quot;)); m3 = Class.forName(&quot;com.zang.jdkproxy.IPerson&quot;).getMethod(&quot;learn&quot;); m2 = Class.forName(&quot;java.lang.Object&quot;).getMethod(&quot;toString&quot;); m0 = Class.forName(&quot;java.lang.Object&quot;).getMethod(&quot;hashCode&quot;); &#125; catch (NoSuchMethodException var2) &#123; throw new NoSuchMethodError(var2.getMessage()); &#125; catch (ClassNotFoundException var3) &#123; throw new NoClassDefFoundError(var3.getMessage()); &#125; &#125;&#125; 可以看到 $Proxy0类继承了Proxy类，里面有一个跟IPerson一样签名的 learn 方法，方法实现中的super.h.invoke(this, m3, (Object[])null);，super.h 对应的是父类的h变量，也就是Proxy.newProxyInstance方法中的InvocationHandler参数： 12345678910111213141516171819202122package java.lang.reflect;//import略public class Proxy implements java.io.Serializable &#123; protected InvocationHandler h; protected Proxy(InvocationHandler h) &#123; Objects.requireNonNull(h); this.h = h; &#125; @CallerSensitive public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) throws IllegalArgumentException &#123; Objects.requireNonNull(h); final Class&lt;?&gt;[] intfs = interfaces.clone(); // 所以这里实际上就是使用了我自己写的InvocationHandler实现类JdkInvocationHandler的invoke方法，当调用 IPerson.learn的时候，其实它是被转发到了 JdkInvocationHandler.invoke。至此，整个魔术过程就透明了。 2.2 手写JDK动态代理使用JDK动态代理的类名和方法名定义以及执行思路，下面来进行手写实现。 创建MyInvocationHandler接口：123456import java.lang.reflect.Method;public interface MyInvocationHandler &#123; public Object invoke(Object proxy, Method method, Object[] args) throws Throwable;&#125; 创建MyProxy类：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147import javax.tools.JavaCompiler;import javax.tools.StandardJavaFileManager;import javax.tools.ToolProvider;import java.io.File;import java.io.FileWriter;import java.lang.reflect.Constructor;import java.lang.reflect.Method;import java.util.HashMap;import java.util.Map;/** * 自己实现的代理类，用来生成字节码文件，并动态加载到JVM中 */public class MyProxy &#123; public static final String ln = &quot;\\r\\n&quot;; /** * 生成代理对象 * @param classLoader 类加载器，用于加载被代理类的类文件 * @param interfaces 被代理类的接口 * @param h 自定义的InvocationHandler接口,用于具体代理方法的执行 * @return 返回被代理后的代理对象 */ public static Object newProxyInstance(MyClassLoader classLoader, Class&lt;?&gt;[] interfaces, MyInvocationHandler h) &#123; try &#123; //1、动态生成源代码.java文件 String src = generateSrc(interfaces); //2、Java文件输出磁盘 String filePath = MyProxy.class.getResource(&quot;&quot;).getPath(); File f = new File(filePath + &quot;$Proxy0.java&quot;); FileWriter fw = new FileWriter(f); fw.write(src); fw.flush(); fw.close(); //3、把生成的.java文件编译成.class文件 //获取Java编译器 JavaCompiler compiler = ToolProvider.getSystemJavaCompiler(); //标注Java文件管理器，用来获取Java字节码文件 StandardJavaFileManager manage = compiler.getStandardFileManager(null, null, null); Iterable iterable = manage.getJavaFileObjects(f); //创建task，通过java字节码文件将类信息加载到JVM中 JavaCompiler.CompilationTask task = compiler.getTask(null, manage, null, null, null, iterable); //开始执行task task.call(); //关闭管理器 manage.close(); //4、编译生成的.class文件加载到JVM中来 Class proxyClass = classLoader.findClass(&quot;$Proxy0&quot;); Constructor c = proxyClass.getConstructor(MyInvocationHandler.class); f.delete(); //5、返回字节码重组以后的新的代理对象 return c.newInstance(h); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 生成代理类的源代码 */ private static String generateSrc(Class&lt;?&gt;[] interfaces) &#123; StringBuffer sb = new StringBuffer(); sb.append(MyProxy.class.getPackage() + &quot;;&quot; + ln); sb.append(&quot;import &quot; + interfaces[0].getName() + &quot;;&quot; + ln); sb.append(&quot;import java.lang.reflect.*;&quot; + ln); sb.append(&quot;public class $Proxy0 implements &quot; + interfaces[0].getName() + &quot;&#123;&quot; + ln); sb.append(&quot;GPInvocationHandler h;&quot; + ln); sb.append(&quot;public $Proxy0(GPInvocationHandler h) &#123; &quot; + ln); sb.append(&quot;this.h = h;&quot;); sb.append(&quot;&#125;&quot; + ln); for (Method m : interfaces[0].getMethods()) &#123; Class&lt;?&gt;[] params = m.getParameterTypes(); StringBuffer paramNames = new StringBuffer(); StringBuffer paramValues = new StringBuffer(); StringBuffer paramClasses = new StringBuffer(); for (int i = 0; i &lt; params.length; i++) &#123; Class clazz = params[i]; String type = clazz.getName(); String paramName = toLowerFirstCase(clazz.getSimpleName()); paramNames.append(type + &quot; &quot; + paramName); paramValues.append(paramName); paramClasses.append(clazz.getName() + &quot;.class&quot;); if (i &gt; 0 &amp;&amp; i &lt; params.length - 1) &#123; paramNames.append(&quot;,&quot;); paramClasses.append(&quot;,&quot;); paramValues.append(&quot;,&quot;); &#125; &#125; sb.append(&quot;public &quot; + m.getReturnType().getName() + &quot; &quot; + m.getName() + &quot;(&quot; + paramNames.toString() + &quot;) &#123;&quot; + ln); sb.append(&quot;try&#123;&quot; + ln); sb.append(&quot;Method m = &quot; + interfaces[0].getName() + &quot;.class.getMethod(\\&quot;&quot; + m.getName() + &quot;\\&quot;,new Class[]&#123;&quot; + paramClasses.toString() + &quot;&#125;);&quot; + ln); sb.append((hasReturnValue(m.getReturnType()) ? &quot;return &quot; : &quot;&quot;) + getCaseCode(&quot;this.h.invoke(this,m,new Object[]&#123;&quot; + paramValues + &quot;&#125;)&quot;, m.getReturnType()) + &quot;;&quot; + ln); sb.append(&quot;&#125;catch(Error _ex) &#123; &#125;&quot;); sb.append(&quot;catch(Throwable e)&#123;&quot; + ln); sb.append(&quot;throw new UndeclaredThrowableException(e);&quot; + ln); sb.append(&quot;&#125;&quot;); sb.append(getReturnEmptyCode(m.getReturnType())); sb.append(&quot;&#125;&quot;); &#125; sb.append(&quot;&#125;&quot; + ln); return sb.toString(); &#125; private static Map&lt;Class, Class&gt; mappings = new HashMap&lt;Class, Class&gt;(); static &#123; mappings.put(int.class, Integer.class); &#125; private static String getReturnEmptyCode(Class&lt;?&gt; returnClass) &#123; if (mappings.containsKey(returnClass)) &#123; return &quot;return 0;&quot;; &#125; else if (returnClass == void.class) &#123; return &quot;&quot;; &#125; else &#123; return &quot;return null;&quot;; &#125; &#125; private static String getCaseCode(String code, Class&lt;?&gt; returnClass) &#123; if (mappings.containsKey(returnClass)) &#123; return &quot;((&quot; + mappings.get(returnClass).getName() + &quot;)&quot; + code + &quot;).&quot; + returnClass.getSimpleName() + &quot;Value()&quot;; &#125; return code; &#125; private static boolean hasReturnValue(Class&lt;?&gt; clazz) &#123; return clazz != void.class; &#125; private static String toLowerFirstCase(String src) &#123; char[] chars = src.toCharArray(); chars[0] += 32; return String.valueOf(chars); &#125;&#125; 创建类加载器MyClassLoader：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import java.io.ByteArrayOutputStream;import java.io.File;import java.io.FileInputStream;public class MyClassLoader extends ClassLoader &#123; private File classPathFile; public MyClassLoader()&#123; String classPath = MyClassLoader.class.getResource(&quot;&quot;).getPath(); this.classPathFile = new File(classPath); &#125; /** * 通过类名称加载类字节码文件到JVM中 * @param name 类名 * @return 类的Class独享 * @throws ClassNotFoundException */ @Override protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; //获取类名 String className = MyClassLoader.class.getPackage().getName() + &quot;.&quot; + name; if(classPathFile != null)&#123; //获取类文件 File classFile = new File(classPathFile,name.replaceAll(&quot;\\\\.&quot;,&quot;/&quot;) + &quot;.class&quot;); if(classFile.exists())&#123; //将类文件转化为字节数组 FileInputStream in = null; ByteArrayOutputStream out = null; try&#123; in = new FileInputStream(classFile); out = new ByteArrayOutputStream(); byte [] buff = new byte[1024]; int len; while ((len = in.read(buff)) != -1)&#123; out.write(buff,0,len); &#125; //调用父类方法生成class实例 return defineClass(className,out.toByteArray(),0,out.size()); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125; &#125; return null; &#125;&#125; 实现并测试123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/*** 要代理的接口*/public interface IPerson &#123; void learn();&#125;/*** 真实调用类*/public class Zhangsan implements IPerson &#123; public void learn() &#123; System.out.println(&quot;==张三学习中间件==&quot;); &#125;&#125;/*** JDK代理类生成*/public class CustomInvocationHandler implements MyInvocationHandler &#123; private IPerson target; public IPerson getInstance(IPerson target)&#123; this.target = target; Class&lt;?&gt; clazz = target.getClass(); return (IPerson) MyProxy.newProxyInstance(new MyClassLoader(),clazz.getInterfaces(),this); &#125; public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; before(); Object result = method.invoke(this.target,args); after(); return result; &#125; private void before() &#123; System.out.println(&quot;事前做好计划&quot;); &#125; private void after() &#123; System.out.println(&quot;事后回顾梳理&quot;); &#125;&#125;/*** 测试*/public class Test &#123; public static void main(String[] args) &#123; CustomInvocationHandler custom = new CustomInvocationHandler(); IPerson zhangsan = custom.getInstance(new Zhangsan()); zhangsan.learn(); &#125;&#125; 至此，手写完成，读者也可自行参照实现。 3.CGLib动态代理API原理分析3.1 CGLib动态代理的使用1234567891011121314151617181920212223242526272829303132import net.sf.cglib.proxy.Enhancer;import net.sf.cglib.proxy.MethodInterceptor;import net.sf.cglib.proxy.MethodProxy;import java.lang.reflect.Method;public class CustomCGlib implements MethodInterceptor &#123; public Object getInstance(Class&lt;?&gt; clazz) throws Exception&#123; //相当于Proxy，代理的工具类 Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(clazz); enhancer.setCallback(this); return enhancer.create(); &#125; public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123; before(); Object obj = methodProxy.invokeSuper(o,objects); after(); return obj; &#125; private void before() &#123; System.out.println(&quot;事前做好计划&quot;); &#125; private void after() &#123; System.out.println(&quot;事后回顾梳理&quot;); &#125;&#125; 这里有一个小细节，CGLib动态代理的目标对象不需要实现任何接口，它是通过动态继承目标对象实现动态代理的，客户端测试代码如下： 12345678910public class CglibTest &#123; public static void main(String[] args) &#123; try &#123; Zhangsan obj = (Zhangsan) new CustomCGlib().getInstance(Zhangsan.class); obj.learn(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 3.2 CGLib动态代理的实现原理CGLib动态代理的实现原理又是怎样的呢？可以在客户端测试代码中加上一句代码，将CGLib动态代理后的.class文件写入磁盘，然后反编译来一探究竟，代码如下： 12345//import net.sf.cglib.core.DebuggingClassWriter;//使用CGLib的代理类可以将内存中的.class文件写入本地磁盘System.setProperty(DebuggingClassWriter.DEBUG_LOCATION_PROPERTY,&quot;E://cglib_proxy_classes&quot;);Zhangsan obj = ···//··· 重新执行代码，在输出目录下会出现三个.class文件，一个是目标（被代理）类的FastClass，一个是代理类，一个是代理类的FastClass。如图： 其中，Zhangsan$$EnhancerByCGLIB$$3d23e0ea.class就是CGLib动态代理生成的代理类，继承了Zhangsan类。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package com.zang.cglibproxy;import java.lang.reflect.Method;import net.sf.cglib.*;public class Zhangsan$$EnhancerByCGLIB$$3d23e0ea extends Zhangsan implements Factory &#123; //··· //传入的MethodInterceptor对象 private MethodInterceptor CGLIB$CALLBACK_0; //目标类的learn方法对象 private static final Method CGLIB$learn$0$Method; //代理类的learn方法对象 private static final MethodProxy CGLIB$learn$0$Proxy; private static final Object[] CGLIB$emptyArgs; //初始化方法，其中部分代码略 static void CGLIB$STATICHOOK1() &#123; CGLIB$THREAD_CALLBACKS = new ThreadLocal(); CGLIB$emptyArgs = new Object[0]; Class var0 = Class.forName(&quot;com.zang.cglibproxy.Zhangsan$$EnhancerByCGLIB$$78b38660&quot;); Class var1; Method[] var10000 = ReflectUtils.findMethods(new String[]&#123;&quot;equals&quot;, &quot;(Ljava/lang/Object;)Z&quot;, &quot;toString&quot;, &quot;()Ljava/lang/String;&quot;, &quot;hashCode&quot;, &quot;()I&quot;, &quot;clone&quot;, &quot;()Ljava/lang/Object;&quot;&#125;, (var1 = Class.forName(&quot;java.lang.Object&quot;)).getDeclaredMethods()); //··· //初始化目标类的learn方法对象 CGLIB$learn$0$Method = ReflectUtils.findMethods(new String[]&#123;&quot;learn&quot;, &quot;()V&quot;&#125;, (var1 = Class.forName(&quot;com.zang.cglibproxy.Zhangsan&quot;)).getDeclaredMethods())[0]; //初始化代理类的learn方法对象 CGLIB$learn$0$Proxy = MethodProxy.create(var1, var0, &quot;()V&quot;, &quot;learn&quot;, &quot;CGLIB$learn$0&quot;); &#125; //这里直接调用Zhangsan#learn final void CGLIB$learn$0() &#123; super.learn(); &#125; public final void learn() &#123; MethodInterceptor var10000 = this.CGLIB$CALLBACK_0; if (var10000 == null) &#123; CGLIB$BIND_CALLBACKS(this); var10000 = this.CGLIB$CALLBACK_0; &#125; if (var10000 != null) &#123; //这里执行拦截器定义逻辑 var10000.intercept(this, CGLIB$learn$0$Method, CGLIB$emptyArgs, CGLIB$learn$0$Proxy); &#125; else &#123; super.learn(); &#125; &#125; //···&#125; 调用过程为：代理对象调用this.learn方法→调用拦截器→methodProxy.invokeSuper()→CGLIB$learn$0→被代理对象learn方法。 1234567package net.sf.cglib.proxy;import java.lang.reflect.Method;public interface MethodInterceptor extends Callback &#123; Object intercept(Object var1, Method var2, Object[] var3, MethodProxy var4) throws Throwable;&#125; 12345678910public class CustomCGlib implements MethodInterceptor &#123; //··· public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123; before(); Object obj = methodProxy.invokeSuper(o,objects); after(); return obj; &#125; //···&#125; MethodInterceptor拦截器就是由MethodProxy的invokeSuper方法调用代理方法的，因此，MethodProxy类中的代码非常关键，下面分析它具体做了什么： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package net.sf.cglib.proxy;import java.lang.reflect.InvocationTargetException;import java.lang.reflect.Method;import net.sf.cglib.*;public class MethodProxy &#123; private Signature sig1; private Signature sig2; private MethodProxy.CreateInfo createInfo; private final Object initLock = new Object(); private volatile MethodProxy.FastClassInfo fastClassInfo; private void init() &#123; if (this.fastClassInfo == null) &#123; synchronized(this.initLock) &#123; if (this.fastClassInfo == null) &#123; MethodProxy.CreateInfo ci = this.createInfo; MethodProxy.FastClassInfo fci = new MethodProxy.FastClassInfo(); //创建目标类的FastClass对象(在缓存中，则取出；没在，则重新生成) fci.f1 = helper(ci, ci.c1); //创建代理类的FastClass对象 fci.f2 = helper(ci, ci.c2); //获取learn方法的索引 fci.i1 = fci.f1.getIndex(this.sig1); //获取CGLIB$learn$0方法的索引 fci.i2 = fci.f2.getIndex(this.sig2); this.fastClassInfo = fci; &#125; &#125; &#125; &#125; public Object invokeSuper(Object obj, Object[] args) throws Throwable &#123; try &#123; //初始化，创建了两个FastClass类对象 this.init(); MethodProxy.FastClassInfo fci = this.fastClassInfo; //这里将直接调用代理类的CGLIB$learn$0方法，而不是通过反射调用 //fci.f2：代理类的FastClass对象，fci.i2为CGLIB$learn$0方法对应的索引，obj为当前的代理类对象，args为learn方法的参数列表 return fci.f2.invoke(fci.i2, obj, args); &#125; catch (InvocationTargetException var4) &#123; throw var4.getTargetException(); &#125; &#125; 上面代码调用获取代理类对应的FastClass，并执行代理方法。还记得之前生成的三个.class文件吗？Zhangsan$$EnhancerByCGLIB$$78b38660$$FastClassByCGLIB$$a8f9873c.class就是代理类的FastClass，Zhangsan$$FastClassByCGLIB$$bcf7b1f4.class就是被代理类的FastClass。 CGLib动态代理执行代理方法的效率之所以比JDK高，是因为CGlib采用了FastClass机制，它的原理简单来说就是：为代理类和被代理类各生成一个类，这个类会为代理类或被代理类的方法分配一个index（int类型）；这个index被当作一个入参，FastClass可以直接定位要调用的方法并直接进行调用，省去了反射调用，因此调用效率比JDK动态代理通过反射调用高（并不绝对，还需参考JDK版本及使用场景来说）。下面来反编译一个FastClass。 123456789101112131415161718192021222324252627282930313233343536public class Zhangsan$$FastClassByCGLIB$$bcf7b1f4 extends FastClass &#123; public Zhangsan$$FastClassByCGLIB$$bcf7b1f4(Class var1) &#123; super(var1); &#125; public int getIndex(Signature var1) &#123; String var10000 = var1.toString(); switch(var10000.hashCode()) &#123; case 1574139569: if (var10000.equals(&quot;learn()V&quot;)) &#123; //learn方法返回0 return 0; &#125; break; case 1826985398: if (var10000.equals(&quot;equals(Ljava/lang/Object;)Z&quot;)) &#123; //··· &#125; &#125; &#125; //根据index获取方法 public Object invoke(int var1, Object var2, Object[] var3) throws InvocationTargetException &#123; Zhangsan var10000 = (Zhangsan)var2; int var10001 = var1; try &#123; switch(var10001) &#123; case 0: //传入index为0则执行learn方法 var10000.learn(); return null; case 1: return new Boolean(var10000.equals(var3[0])); //··· FastClass并不是跟代理类一起生成的，而是在第一次执行MethodProxy的invoke或invokeSuper方法时生成的，并被放在了缓存中。 4.总结通过上面的分析，相信会对两种动态代理的实现原理有一个深入的认识，总结性比较两者的区别如下： JDK动态代理实现了被代理对象的接口，CGLib动态代理继承了被代理对象。 JDK动态代理和CGLib动态代理都在运行期生成字节码，JDK动态代理直接写Class字节码，CGLib动态代理使用ASM框架写Class字节码。CGLib动态代理实现更复杂，生成代理类比JDK动态代理效率低。 JDK动态代理调用代理方法是通过反射机制调用的，CGLib动态代理是通过FastClass机制直接调用方法的，CGLib动态代理的执行效率更高。","categories":[{"name":"技术","slug":"技术","permalink":"https://xssdpgy.github.io/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"源码分析","slug":"源码分析","permalink":"https://xssdpgy.github.io/tags/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"name":"设计模式","slug":"设计模式","permalink":"https://xssdpgy.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"动态代理","slug":"动态代理","permalink":"https://xssdpgy.github.io/tags/%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86/"}]},{"title":"SpringBoot异步方法优化处理提高响应速度","slug":"SpringBoot异步方法优化处理提高响应速度","date":"2022-04-21T02:48:24.000Z","updated":"2022-09-11T15:43:16.281Z","comments":true,"path":"2022/04/21/SpringBoot异步方法优化处理提高响应速度/","link":"","permalink":"https://xssdpgy.github.io/2022/04/21/SpringBoot%E5%BC%82%E6%AD%A5%E6%96%B9%E6%B3%95%E4%BC%98%E5%8C%96%E5%A4%84%E7%90%86%E6%8F%90%E9%AB%98%E5%93%8D%E5%BA%94%E9%80%9F%E5%BA%A6/","excerpt":"1.前言日常开发中，对于串行化的任务适当解耦耗时操作和业务逻辑，在保证结果准确性的前提下，使用异步方法适当进行并行化改造，可以提高接口响应速度，提升使用体验。 如下抽象的串行化工作流程： 业务查询，首先登记记录record[cost 3s]，之后依次执行searchA[cost 1s]、searchB[cost 2s]、searchC[cost 2s]分别得到变量a、b、c，返回结果fx(a,b,c)[计算耗时可忽略不记]。代码如下：","text":"1.前言日常开发中，对于串行化的任务适当解耦耗时操作和业务逻辑，在保证结果准确性的前提下，使用异步方法适当进行并行化改造，可以提高接口响应速度，提升使用体验。 如下抽象的串行化工作流程： 业务查询，首先登记记录record[cost 3s]，之后依次执行searchA[cost 1s]、searchB[cost 2s]、searchC[cost 2s]分别得到变量a、b、c，返回结果fx(a,b,c)[计算耗时可忽略不记]。代码如下： 123456789101112131415161718192021222324252627282930import com.zang.async.service.AsyncCaseService;import lombok.extern.slf4j.Slf4j;import org.springframework.web.bind.annotation.PostMapping;import org.springframework.web.bind.annotation.RestController;import javax.annotation.Resource;import java.time.Duration;import java.time.Instant;@Slf4j@RestControllerpublic class AsyncCaseController &#123; @Resource private AsyncCaseService asyncCaseService; @PostMapping(&quot;/search/sync-test&quot;) public int syncSearch()&#123; log.info(&quot;========test start=========&quot;); Instant start = Instant.now(); asyncCaseService.record(); int a = asyncCaseService.searchA(); int b = asyncCaseService.searchB(); int c = asyncCaseService.searchC(); int result = a+b+c; Instant end = Instant.now(); log.info(&quot;========test end=========cost time is &#123;&#125; seconds&quot;, Duration.between(start,end).getSeconds()); return result; &#125; ··· 123456789101112131415161718import org.springframework.stereotype.Service;@Servicepublic class AsyncCaseServiceImpl implements AsyncCaseService&#123; @Override public int searchA() &#123; try &#123; Thread.sleep(1000);//模拟业务处理耗时 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return 1; &#125; @Override public int searchB() &#123; //其他方法类似 执行结果： 122022-04-21 13:32:47.739 INFO 22764 --- [nio-8089-exec-2] com.zang.async.web.AsyncCaseController : ========test start=========2022-04-21 13:32:55.762 INFO 22764 --- [nio-8089-exec-2] com.zang.async.web.AsyncCaseController : ========test end=========cost time is 8 seconds 经过分析，可以看到三个查询方法可以并行执行，等待都产生结果执行fx(a,b,c)，record方法执行的顺序和完成度不影响结果的返回，可以使用异步任务执行。改造逻辑抽象如下： 之后就代码实现展开阐述。 2.SpringBoot中的异步方法支持SpringBoot已经提供了异步方法支持注解，因此不需要我们自己去创建维护线程或者线程池来异步的执行方法。 主要依靠两个注解： 12@EnableAsync // 使用异步方法时需要提前开启(在启动类上或配置类上)@Async // 被async注解修饰的方法由SpringBoot默认线程池(SimpleAsyncTaskExecutor)执行 2.1 获取(有返回值)异步方法的返回值对于有返回值的异步方法，可使用java.util.concurrent.Future类及其子类来接收异步方法返回值。 1234567891011121314151617181920import org.springframework.scheduling.annotation.Async;import org.springframework.scheduling.annotation.AsyncResult;import org.springframework.stereotype.Service;import java.util.concurrent.Future;@Servicepublic class AsyncCaseServiceImpl implements AsyncCaseService&#123; @Async @Override public Future&lt;Integer&gt; searchA() &#123; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return new AsyncResult&lt;&gt;(1); &#125; //略 无返回值异步方法的异常捕获见3.3。 2.2 异步任务并行控制接上节，在对Service中有返回值的方法进行异步改造的同时，业务处理侧需要添加并行控制，使并行的异步都返回结果才进行下一步操作： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import com.zang.async.service.AsyncCaseService;import lombok.extern.slf4j.Slf4j;import org.springframework.web.bind.annotation.PostMapping;import org.springframework.web.bind.annotation.RestController;import javax.annotation.Resource;import java.time.Duration;import java.time.Instant;import java.util.concurrent.Future;@Slf4j@RestControllerpublic class AsyncCaseController &#123; @Resource private AsyncCaseService asyncCaseService; @PostMapping(&quot;/search/async-test&quot;) public int asyncSearch() &#123; log.info(&quot;========test start=========&quot;); Instant start = Instant.now(); asyncCaseService.record(); Future&lt;Integer&gt; searchAFuture = asyncCaseService.searchA(); Future&lt;Integer&gt; searchBFuture = asyncCaseService.searchB(); Future&lt;Integer&gt; searchCFuture = asyncCaseService.searchC(); while (true) &#123; if (searchAFuture.isDone() &amp;&amp; searchBFuture.isDone() &amp;&amp; searchCFuture.isDone()) &#123; break; &#125; if (searchAFuture.isCancelled() || searchBFuture.isCancelled() || searchCFuture.isCancelled()) &#123; log.info(&quot;async work has cancelled , break&quot;); break; &#125; try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; int a = 0, b = 0, c = 0; try &#123; a = searchAFuture.get(); b = searchBFuture.get(); c = searchCFuture.get(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; int result = a + b + c; Instant end = Instant.now(); log.info(&quot;========test end=========cost time is &#123;&#125; seconds&quot;, Duration.between(start, end).getSeconds()); return result; &#125;&#125; 结果： 122022-04-21 14:23:35.486 INFO 19912 --- [nio-8089-exec-4] com.zang.async.web.AsyncCaseController : ========test start=========2022-04-21 14:23:37.516 INFO 19912 --- [nio-8089-exec-4] com.zang.async.web.AsyncCaseController : ========test end=========cost time is 2 seconds 3.自定义线程池执行异步方法@Async使用了线程池org.springframework.core.task.SimpleAsyncTaskExecutor来执行我们的异步方法，实际开发中我们也可以自定义自己的线程池，便于对线程池进行合理配置。 3.1 自定义线程池12345678910111213141516171819202122232425262728import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.scheduling.annotation.EnableAsync;import org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor;import java.util.concurrent.Executor;import java.util.concurrent.ThreadPoolExecutor;@EnableAsync@Configurationpublic class AsyncThreadPoolConfigure &#123; @Bean(&quot;asyncThreadPoolTaskExecutor&quot;) public Executor asyncThreadPoolTaskExecutor() &#123; ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); executor.setCorePoolSize(4); executor.setMaxPoolSize(4); executor.setQueueCapacity(10); executor.setKeepAliveSeconds(60); executor.setThreadNamePrefix(&quot;async-task-executor&quot;); executor.setThreadGroupName(&quot;async-task-executor-group&quot;); executor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy()); // 所有任务结束后关闭线程池 //executor.setWaitForTasksToCompleteOnShutdown(true); executor.initialize(); return executor; &#125;&#125; 3.2 在@Async注解上指定执行的线程池12345@Async(&quot;asyncThreadPoolTaskExecutor&quot;)@Overridepublic Future&lt;Integer&gt; searchA() &#123; try &#123; //略 以上，自定义线程池执行异步方法即完成。 3.3 自定义线程池监控自定义的线程池配置的参数是否合理往往使人摸不着头脑，实际上，线程池执行器org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor为Spring自带的，在测试中可以创建新执行器，继承该执行器，重写submit方法，对其增加监控，从而查看线程池状态，得到合适的线程池配置。 123456789101112public class MonitorThreadPoolExecutor extends ThreadPoolTaskExecutor &#123; public void monitor()&#123; log.info(&quot;**** getActiveCount==&#123;&#125;,getPoolSize==&#123;&#125;,getLargestPoolSize==&#123;&#125;,getTaskCount==&#123;&#125;,getCompletedTaskCount==&#123;&#125;,getQueue==&#123;&#125; ***&quot;,this.getThreadPoolExecutor().getActiveCount(),this.getThreadPoolExecutor().getPoolSize(),this.getThreadPoolExecutor().getLargestPoolSize(),this.getThreadPoolExecutor().getTaskCount(),this.getThreadPoolExecutor().getCompletedTaskCount(),this.getThreadPoolExecutor().getQueue().size()); &#125; @Override public &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task) &#123; monitor(); return super.submit(task); &#125;&#125; 在3.1自定义线程池时创建该监控执行器即可。 3.3 无返回值异步方法的异常捕获以实现org.springframework.scheduling.annotation.AsyncConfigurer接口的getAsyncExecutor方法和getAsyncUncaughtExceptionHandler方法改造配置类。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import lombok.extern.slf4j.Slf4j;import org.springframework.aop.interceptor.AsyncUncaughtExceptionHandler;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.scheduling.annotation.AsyncConfigurer;import org.springframework.scheduling.annotation.EnableAsync;import org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor;import java.lang.reflect.Method;import java.util.concurrent.Executor;import java.util.concurrent.ThreadPoolExecutor;@Slf4j@EnableAsync@Configurationpublic class AsyncThreadPoolConfigure implements AsyncConfigurer &#123; //线程池创建方法为重写 getAsyncExecutor @Bean(&quot;asyncThreadPoolTaskExecutor&quot;) @Override public Executor getAsyncExecutor() &#123; ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); executor.setCorePoolSize(4); executor.setMaxPoolSize(4); executor.setQueueCapacity(10); executor.setKeepAliveSeconds(60); executor.setThreadNamePrefix(&quot;async-task-executor&quot;); executor.setThreadGroupName(&quot;async-task-executor-group&quot;); executor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy()); // 所有任务结束后关闭线程池 executor.setWaitForTasksToCompleteOnShutdown(true); executor.initialize(); return executor; &#125; @Override public AsyncUncaughtExceptionHandler getAsyncUncaughtExceptionHandler() &#123; return new AsyncExceptionHandler(); &#125; public class AsyncExceptionHandler implements AsyncUncaughtExceptionHandler &#123; @Override public void handleUncaughtException(Throwable throwable, Method method, Object... obj) &#123; log.error(&quot;Exception message is &#123;&#125;&quot;, throwable.getMessage()); log.error(&quot;Method name is &#123;&#125; &quot;, method.getName()); for (Object param : obj) &#123; log.error(&quot;Parameter value - &#123;&#125;&quot;, param); &#125; &#125; &#125; 表现如下： 1234567891011@Async(&quot;asyncThreadPoolTaskExecutor&quot;) @Override public void record() &#123; try &#123; Thread.sleep(3000); log.info(&quot;current thread name is &#123;&#125;&quot;,Thread.currentThread().getName()); throw new RuntimeException(&quot;network not connect &quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; 控制台： 123452022-04-21 15:34:14.931 INFO 16596 --- [nio-8089-exec-1] com.zang.async.web.AsyncCaseController : ========test start=========2022-04-21 15:34:16.965 INFO 16596 --- [nio-8089-exec-1] com.zang.async.web.AsyncCaseController : ========test end=========cost time is 2 seconds2022-04-21 15:34:17.939 INFO 16596 --- [-task-executor1] c.z.async.service.AsyncCaseServiceImpl : current thread name is async-task-executor12022-04-21 15:34:17.940 ERROR 16596 --- [-task-executor1] c.z.a.c.AsyncThreadPoolConfigure : Exception message is network not connect 2022-04-21 15:34:17.941 ERROR 16596 --- [-task-executor1] c.z.a.c.AsyncThreadPoolConfigure : Method name is record 4.一些思考异步方法的集成极为方便，可以有效提高接口响应速度，但是使用过程中要注意合理的分析业务逻辑及服务器资源承载能力，不可滥用。 对于强一致性的业务，需要注意，异步方法执行失败对于前部分的已执行的非异步操作是无影响的，因此在该场景异步并不可靠； 此外，对于并发量过大的任务，异步线程池的队列缓存也较为消耗服务器资源，需要合理规划，必要时建议采用更为可靠的消息队列等中间件。","categories":[{"name":"技术","slug":"技术","permalink":"https://xssdpgy.github.io/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"异步","slug":"异步","permalink":"https://xssdpgy.github.io/tags/%E5%BC%82%E6%AD%A5/"},{"name":"优化","slug":"优化","permalink":"https://xssdpgy.github.io/tags/%E4%BC%98%E5%8C%96/"}]},{"title":"二进制方式安装k8s集群","slug":"二进制方式安装k8s集群","date":"2022-03-30T07:51:45.000Z","updated":"2022-09-11T15:43:16.282Z","comments":true,"path":"2022/03/30/二进制方式安装k8s集群/","link":"","permalink":"https://xssdpgy.github.io/2022/03/30/%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E5%AE%89%E8%A3%85k8s%E9%9B%86%E7%BE%A4/","excerpt":"使用三台服务器搭建k8s集群，集群服务器地址规划如下：","text":"使用三台服务器搭建k8s集群，集群服务器地址规划如下： IP hostname 备注 192.168.206.128 master 主节点 192.168.206.129 node1 从节点 192.168.206.130 node2 从节点 1.环境配置1.1 修改主机名master: 1hostnamectl set-hostname master node1: 1hostnamectl set-hostname node1 node2: 1hostnamectl set-hostname 1.2 关闭防火墙（all）12systemctl stop firewalldsystemctl disable firewalld 1.3 关闭selinux（all）12setenforce 0 # 临时关闭sed -i &#x27;s/SELINUX=enforcing/SELINUX=disabled/g&#x27; /etc/selinux/config # 永久关闭 1.4 关闭swap（all）123swapoff -a # 临时关闭；关闭swap主要是为了性能考虑sed -ri &#x27;s/.*swap.*/#&amp;/&#x27; /etc/fstabfree # 查看内存，swap为0则为关闭 1.5 将桥接的IPv4流量传递到iptables的链（all）1234cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt; EOFnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOF 1sysctl --system 1.6 添加主机名与IP对应的关系 ( master )12345cat &gt;&gt; /etc/hosts &lt;&lt; EOF 192.168.206.128 master192.168.206.129 node1192.168.206.130 node2EOF 2.准备 cfssl 证书生成工具 ( master )cfssl 是一个开源的证书管理工具，使用 json 文件生成证书，相比 openssl 更方便使用。 找任意一台服务器操作，这里用 Master 节点。 12345678wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64chmod +x cfssl_linux-amd64 cfssljson_linux-amd64 cfssl-certinfo_linux-amd64mv cfssl_linux-amd64 /usr/local/bin/cfsslmv cfssljson_linux-amd64 /usr/local/bin/cfssljsonmv cfssl-certinfo_linux-amd64 /usr/bin/cfssl-certinfochmod +x /usr/bin/cfssl* 2.1 生成 Etcd 证书 （1）自签证书颁发机构（CA） 创建工作目录123mkdir -p ~/TLS/&#123;etcd,k8s&#125;cd TLS/etcd 2.2 自签CA1234567891011121314151617181920cat &gt; ca-config.json &lt;&lt; EOF&#123; &quot;signing&quot;: &#123; &quot;default&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot; &#125;, &quot;profiles&quot;: &#123; &quot;www&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot;, &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot; ] &#125; &#125; &#125;&#125;EOF 12345678910111213141516cat &gt; ca-csr.json &lt;&lt; EOF&#123; &quot;CN&quot;: &quot;etcd CA&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;Beijing&quot;, &quot;ST&quot;: &quot;Beijing&quot; &#125; ]&#125;EOF 2.3 生成CA证书1cfssl gencert -initca ca-csr.json | cfssljson -bare ca - 12[root@master etcd]# ls ca*pem #查看ca-key.pem ca.pem 2.4 使用自签 CA 签发 Etcd HTTPS 证书 创建证书申请文件：(修改对应的master和node的IP地址)123456789101112131415161718192021cat &gt; server-csr.json &lt;&lt; EOF&#123; &quot;CN&quot;: &quot;etcd&quot;, &quot;hosts&quot;: [ &quot;192.168.206.128&quot;, &quot;192.168.206.129&quot;, &quot;192.168.206.130&quot; ], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;ST&quot;: &quot;BeiJing&quot; &#125; ]&#125;EOF 2.5 生成SERVER证书1cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=www server-csr.json | cfssljson -bare server 12[root@master etcd]# ls server*pem #查看server-key.pem server.pem 3.部署etcd集群3.1 下载12下载地址：https://github.com/etcd-io/etcd/releases版本：3.4.14 以下在master 上操作，为简化操作，完成后将master 生成的所有文件拷贝到node1 和node2。 3.2 创建工作目录并解压二进制包123mkdir /opt/etcd/&#123;bin,cfg,ssl&#125; -ptar zxvf etcd-v3.4.14-linux-amd64.tar.gzmv etcd-v3.4.14-linux-amd64/&#123;etcd,etcdctl&#125; /opt/etcd/bin/ 3.3 创建etcd.conf12345678910111213cat &gt; /opt/etcd/cfg/etcd.conf &lt;&lt; EOF#[Member]ETCD_NAME=&quot;etcd-1&quot;ETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot;ETCD_LISTEN_PEER_URLS=&quot;https://192.168.206.128:2380&quot;ETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.206.128:2379&quot;#[Clustering]ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.206.128:2380&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;https://192.168.206.128:2379&quot;ETCD_INITIAL_CLUSTER=&quot;etcd-1=https://192.168.206.128:2380,etcd-2=https://192.168.206.129:2380,etcd-3=https://192.168.206.130:2380&quot;ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;EOF ETCD_NAME：节点名称，集群中唯一 ETCD_DATA_DIR：数据目录 ETCD_LISTEN_PEER_URLS：集群通信监听地址 ETCD_LISTEN_CLIENT_URLS：客户端访问监听地址 ETCD_INITIAL_ADVERTISE_PEER_URLS：集群通告地址 ETCD_ADVERTISE_CLIENT_URLS：客户端通告地址 ETCD_INITIAL_CLUSTER：集群节点地址 ETCD_INITIAL_CLUSTER_TOKEN：集群 Token ETCD_INITIAL_CLUSTER_STATE：加入集群的当前状态，new 是新集群，existing 表示加入 已有集群 3.4 创建etcd.service12345678910111213141516171819202122cat &gt; /usr/lib/systemd/system/etcd.service &lt;&lt; EOF[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notifyEnvironmentFile=/opt/etcd/cfg/etcd.confExecStart=/opt/etcd/bin/etcd \\--cert-file=/opt/etcd/ssl/server.pem \\--key-file=/opt/etcd/ssl/server-key.pem \\--peer-cert-file=/opt/etcd/ssl/server.pem \\--peer-key-file=/opt/etcd/ssl/server-key.pem \\--trusted-ca-file=/opt/etcd/ssl/ca.pem \\--peer-trusted-ca-file=/opt/etcd/ssl/ca.pem \\--logger=zapRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF 3.5 拷贝上一步生成的证书到配置路径 1cp ~/TLS/etcd/ca*pem ~/TLS/etcd/server*pem /opt/etcd/ssl/ 3.6 将master 生成的所有文件拷贝到node1 和node212345scp -r /opt/etcd/ root@192.168.206.129:/opt/scp /usr/lib/systemd/system/etcd.service root@192.168.206.129:/usr/lib/systemd/system/scp -r /opt/etcd/ root@192.168.206.130:/opt/scp /usr/lib/systemd/system/etcd.service root@192.168.206.130:/usr/lib/systemd/system/ 分别修改 etcd.conf 配置文件中的节点名称和当前服务器 IP：(node1改为 etcd-2，node2 改为 etcd-3) 3.7 启动并设置开机启动1234# 三台同时执行systemctl daemon-reloadsystemctl start etcdsystemctl enable etcd 查看状态： 1234/opt/etcd/bin/etcdctl --cacert=/opt/etcd/ssl/ca.pem --cert=/opt/etcd/ssl/server.pem --key=/opt/etcd/ssl/server-key.pem --endpoints=&quot;https://192.168.206.128:2379,https://192.168.206.129:2379,https://192.168.206.130:2379&quot; endpoint health#可视化展示/opt/etcd/bin/etcdctl --cacert=/opt/etcd/ssl/ca.pem --cert=/opt/etcd/ssl/server.pem --key=/opt/etcd/ssl/server-key.pem --endpoints=&quot;https://192.168.206.128:2379,https://192.168.206.129:2379,https://192.168.206.130:2379&quot; endpoint status --write-out=table 4.安装docker（all）4.1 下载12下载地址：https://download.docker.com/linux/static/stable/x86_64/版本：19.03.9 4.2 解压及安装12tar zxvf docker-19.03.9.tgz mv docker/* /usr/bin 4.3 systemd 管理 docker12345678910111213141516171819202122cat &gt; /usr/lib/systemd/system/docker.service &lt;&lt; EOF[Unit]Description=Docker Application Container EngineDocumentation=https://docs.docker.comAfter=network-online.target firewalld.serviceWants=network-online.target[Service]Type=notifyExecStart=/usr/bin/dockerdExecReload=/bin/kill -s HUP $MAINPIDLimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinityTimeoutStartSec=0Delegate=yesKillMode=processRestart=on-failureStartLimitBurst=3StartLimitInterval=60s[Install]WantedBy=multi-user.targetEOF 4.4 配置阿里云加速123456mkdir /etc/dockercat &gt; /etc/docker/daemon.json &lt;&lt; EOF&#123; &quot;registry-mirrors&quot;: [&quot;https://b9pmyelo.mirror.aliyuncs.com&quot;]&#125;EOF 4.5 启动并设置开机启动123systemctl daemon-reloadsystemctl start dockersystemctl enable docker 4.6 查询是否安装成功12[root@master etcd]# docker -vDocker version 19.03.9, build 9d988398e7 5.部署Master Node（master）5.1 生成 kube-apiserver 证书 自签证书颁发机构（CA）1cd TLS/k8s 1234567891011121314151617181920cat &gt; ca-config.json &lt;&lt; EOF&#123; &quot;signing&quot;: &#123; &quot;default&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot; &#125;, &quot;profiles&quot;: &#123; &quot;kubernetes&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot;, &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot; ] &#125; &#125; &#125;&#125;EOF 123456789101112131415161718cat &gt; ca-csr.json &lt;&lt; EOF&#123; &quot;CN&quot;: &quot;kubernetes&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;Beijing&quot;, &quot;ST&quot;: &quot;Beijing&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125;EOF 5.2 生成CA证书1cfssl gencert -initca ca-csr.json | cfssljson -bare ca - 12[root@master k8s]# ls ca*pem #查看ca-key.pem ca.pem 5.3 使用自签 CA 签发 kube-apiserver HTTPS 证书 创建证书申请文件12345678910111213141516171819202122232425262728293031cat &gt; server-csr.json &lt;&lt; EOF&#123; &quot;CN&quot;: &quot;kubernetes&quot;, &quot;hosts&quot;: [ &quot;10.0.0.1&quot;, &quot;127.0.0.1&quot;, &quot;192.168.206.128&quot;, &quot;192.168.206.129&quot;, &quot;192.168.206.130&quot;, &quot;192.168.206.131&quot;, &quot;kubernetes&quot;, &quot;kubernetes.default&quot;, &quot;kubernetes.default.svc&quot;, &quot;kubernetes.default.svc.cluster&quot;, &quot;kubernetes.default.svc.cluster.local&quot; ], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;ST&quot;: &quot;BeiJing&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125;EOF 注：192.168.206.131为预留出的IP。 5.4 生成SERVER证书1cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes server-csr.json | cfssljson -bare server 12[root@master k8s]# ls server*pem #查看server-key.pem server.pem 5.5 下载k8s安装包并解压12下载地址：https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.18.md#server-binaries版本：1.18.20 (压缩包名：kubernetes-server-linux-amd64.tar.gz) 12345mkdir -p /opt/kubernetes/&#123;bin,cfg,ssl,logs&#125;tar zxvf kubernetes-server-linux-amd64.tar.gzcd kubernetes/server/bincp kube-apiserver kube-scheduler kube-controller-manager /opt/kubernetes/bincp kubectl /usr/bin/ 5.6 部署kube-apiserver123456789101112131415161718192021222324252627282930cat &gt; /opt/kubernetes/cfg/kube-apiserver.conf &lt;&lt; EOFKUBE_APISERVER_OPTS=&quot;--logtostderr=false \\\\--v=2 \\\\--log-dir=/opt/kubernetes/logs \\\\--etcd-servers=https://192.168.206.128:2379,https://192.168.206.129:2379,https://192.168.206.130:2379 \\\\--bind-address=192.168.206.128 \\\\--secure-port=6443 \\\\--advertise-address=192.168.206.128 \\\\--allow-privileged=true \\\\--service-cluster-ip-range=10.0.0.0/24 \\\\--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,NodeRestriction \\\\--authorization-mode=RBAC,Node \\\\--enable-bootstrap-token-auth=true \\\\--token-auth-file=/opt/kubernetes/cfg/token.csv \\\\--service-node-port-range=30000-32767 \\\\--kubelet-client-certificate=/opt/kubernetes/ssl/server.pem \\\\--kubelet-client-key=/opt/kubernetes/ssl/server-key.pem \\\\--tls-cert-file=/opt/kubernetes/ssl/server.pem \\\\--tls-private-key-file=/opt/kubernetes/ssl/server-key.pem \\\\--client-ca-file=/opt/kubernetes/ssl/ca.pem \\\\--service-account-key-file=/opt/kubernetes/ssl/ca-key.pem \\\\--etcd-cafile=/opt/etcd/ssl/ca.pem \\\\--etcd-certfile=/opt/etcd/ssl/server.pem \\\\--etcd-keyfile=/opt/etcd/ssl/server-key.pem \\\\--audit-log-maxage=30 \\\\--audit-log-maxbackup=3 \\\\--audit-log-maxsize=100 \\\\--audit-log-path=/opt/kubernetes/logs/k8s-audit.log&quot;EOF 上面两个\\ \\ 第一个是转义符，第二个是换行符，使用转义符是为了使用 EOF 保留换行符。 –logtostderr：启用日志 —v：日志等级 –log-dir：日志目录 –etcd-servers：etcd 集群地址 –bind-address：监听地址 –secure-port：https 安全端口 –advertise-address：集群通告地址 –allow-privileged：启用授权 –service-cluster-ip-range：Service 虚拟 IP 地址段 –enable-admission-plugins：准入控制模块 –authorization-mode：认证授权，启用 RBAC 授权和节点自管理 –enable-bootstrap-token-auth：启用 TLS bootstrap 机制 –token-auth-file：bootstrap token 文件 –service-node-port-range：Service nodeport 类型默认分配端口范围 –kubelet-client-xxx：apiserver 访问 kubelet 客户端证书 –tls-xxx-file：apiserver https 证书 –etcd-xxxfile：连接 Etcd 集群证书 –audit-log-xxx：审计日志 5.7 把生成的证书拷贝到配置文件中的路径1cp ~/TLS/k8s/ca*pem ~/TLS/k8s/server*pem /opt/kubernetes/ssl/ 5.8 创建上述配置文件中 token 文件123cat &gt; /opt/kubernetes/cfg/token.csv &lt;&lt; EOFc47ffb939f5ca36231d9e3121a252940,kubelet-bootstrap,10001,&quot;system:node-bootstrapper&quot;EOF 格式：token，用户名，UID，用户组 token 也可自行生成替换： 1head -c 16 /dev/urandom | od -An -t x | tr -d &#x27; &#x27; 5.9 systemd 管理 apiserver1234567891011cat &gt; /usr/lib/systemd/system/kube-apiserver.service &lt;&lt; EOF[Unit]Description=Kubernetes API ServerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=/opt/kubernetes/cfg/kube-apiserver.confExecStart=/opt/kubernetes/bin/kube-apiserver \\$KUBE_APISERVER_OPTSRestart=on-failure[Install]WantedBy=multi-user.targetEOF 启动并设置开机启动 123systemctl daemon-reloadsystemctl start kube-apiserversystemctl enable kube-apiserver 5.10 授权 kubelet-bootstrap 用户允许请求证书123kubectl create clusterrolebinding kubelet-bootstrap \\--clusterrole=system:node-bootstrapper \\--user=kubelet-bootstrap 5.11 部署 kube-controller-manager12345678910111213141516cat &gt; /opt/kubernetes/cfg/kube-controller-manager.conf &lt;&lt; EOFKUBE_CONTROLLER_MANAGER_OPTS=&quot;--logtostderr=false \\\\--v=2 \\\\--log-dir=/opt/kubernetes/logs \\\\--leader-elect=true \\\\--master=127.0.0.1:8080 \\\\--bind-address=127.0.0.1 \\\\--allocate-node-cidrs=true \\\\--cluster-cidr=10.244.0.0/16 \\\\--service-cluster-ip-range=10.0.0.0/24 \\\\--cluster-signing-cert-file=/opt/kubernetes/ssl/ca.pem \\\\--cluster-signing-key-file=/opt/kubernetes/ssl/ca-key.pem \\\\--root-ca-file=/opt/kubernetes/ssl/ca.pem \\\\--service-account-private-key-file=/opt/kubernetes/ssl/ca-key.pem \\\\--experimental-cluster-signing-duration=87600h0m0s&quot;EOF –master：通过本地非安全本地端口 8080 连接 apiserver –leader-elect：当该组件启动多个时，自动选举（HA） –cluster-signing-cert-file&#x2F;–cluster-signing-key-file：自动为 kubelet 颁发证书的 CA，与 apiserver 保持一致 5.12 systemd 管理 controller-manager1234567891011cat &gt; /usr/lib/systemd/system/kube-controller-manager.service &lt;&lt; EOF[Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=/opt/kubernetes/cfg/kube-controller-manager.confExecStart=/opt/kubernetes/bin/kube-controller-manager \\$KUBE_CONTROLLER_MANAGER_OPTSRestart=on-failure[Install]WantedBy=multi-user.targetEOF 启动并设置开机启动 123systemctl daemon-reloadsystemctl start kube-controller-managersystemctl enable kube-controller-manager 5.13 部署 kube-scheduler12345678cat &gt; /opt/kubernetes/cfg/kube-scheduler.conf &lt;&lt; EOFKUBE_SCHEDULER_OPTS=&quot;--logtostderr=false \\--v=2 \\--log-dir=/opt/kubernetes/logs \\--leader-elect \\--master=127.0.0.1:8080 \\--bind-address=127.0.0.1&quot;EOF –master：通过本地非安全本地端口 8080 连接 apiserver –leader-elect：当该组件启动多个时，自动选举（HA） 1234567891011cat &gt; /usr/lib/systemd/system/kube-scheduler.service &lt;&lt; EOF[Unit]Description=Kubernetes SchedulerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=/opt/kubernetes/cfg/kube-scheduler.confExecStart=/opt/kubernetes/bin/kube-scheduler \\$KUBE_SCHEDULER_OPTSRestart=on-failure[Install]WantedBy=multi-user.targetEOF 启动并设置开机启动 123systemctl daemon-reloadsystemctl start kube-schedulersystemctl enable kube-scheduler 5.14 查看集群状态1kubectl get cs 6.部署Worker Node（两个node同步执行）6.1k8s安装包解压安装12345mkdir -p /opt/kubernetes/&#123;bin,cfg,ssl,logs&#125;tar zxvf kubernetes-server-linux-amd64.tar.gzcd kubernetes/server/bincp kubelet kube-proxy /opt/kubernetes/bincp kubectl /usr/bin/ 6.2 配置kubelet123456789101112cat &gt; /opt/kubernetes/cfg/kubelet.conf &lt;&lt; EOFKUBELET_OPTS=&quot;--logtostderr=false \\\\--v=2 \\\\--log-dir=/opt/kubernetes/logs \\\\--hostname-override=m1 \\\\--network-plugin=cni \\\\--kubeconfig=/opt/kubernetes/cfg/kubelet.kubeconfig \\\\--bootstrap-kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig \\\\--config=/opt/kubernetes/cfg/kubelet-config.yml \\\\--cert-dir=/opt/kubernetes/ssl \\\\--pod-infra-container-image=lizhenliang/pause-amd64:3.0&quot;EOF –hostname-override：显示名称，集群中唯一 –network-plugin：启用CNI –kubeconfig：空路径，会自动生成，后面用于连接apiserver –bootstrap-kubeconfig：首次启动向apiserver申请证书 –config：配置参数文件 –cert-dir：kubelet证书生成目录 –pod-infra-container-image：管理Pod网络容器的镜像 1234567891011121314151617181920212223242526272829303132cat &gt; /opt/kubernetes/cfg/kubelet-config.yml &lt;&lt; EOFkind: KubeletConfigurationapiVersion: kubelet.config.k8s.io/v1beta1address: 0.0.0.0port: 10250readOnlyPort: 10255cgroupDriver: cgroupfsclusterDNS:- 10.0.0.2clusterDomain: cluster.local failSwapOn: falseauthentication: anonymous: enabled: false webhook: cacheTTL: 2m0s enabled: true x509: clientCAFile: /opt/kubernetes/ssl/ca.pem authorization: mode: Webhook webhook: cacheAuthorizedTTL: 5m0s cacheUnauthorizedTTL: 30sevictionHard: imagefs.available: 15% memory.available: 100Mi nodefs.available: 10% nodefs.inodesFree: 5%maxOpenFiles: 1000000maxPods: 110EOF 6.3 将master一些配置文件拷贝到node节点上12scp -r /opt/kubernetes/ssl root@192.168.206.129:/opt/kubernetesscp -r /opt/kubernetes/ssl root@192.168.206.130:/opt/kubernetes 6.4 生成bootstrap.kubeconfig文件12KUBE_APISERVER=&quot;https://192.168.206.128:6443&quot; # apiserver IP:PORTTOKEN=&quot;c47ffb939f5ca36231d9e3121a252940&quot; # 与token.csv里保持一致 上面两个变量需要根据自己情况设置，赋到脚本对应位置执行： 12345678910111213kubectl config set-cluster kubernetes \\ --certificate-authority=/opt/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=$&#123;KUBE_APISERVER&#125; \\ --kubeconfig=bootstrap.kubeconfigkubectl config set-credentials &quot;kubelet-bootstrap&quot; \\ --token=$&#123;TOKEN&#125; \\ --kubeconfig=bootstrap.kubeconfigkubectl config set-context default \\ --cluster=kubernetes \\ --user=&quot;kubelet-bootstrap&quot; \\ --kubeconfig=bootstrap.kubeconfigkubectl config use-context default --kubeconfig=bootstrap.kubeconfig 1mv bootstrap.kubeconfig /opt/kubernetes/cfg 6.5 systemd管理kubelet123456789101112cat &gt; /usr/lib/systemd/system/kubelet.service &lt;&lt; EOF[Unit]Description=Kubernetes KubeletAfter=docker.service[Service]EnvironmentFile=/opt/kubernetes/cfg/kubelet.confExecStart=/opt/kubernetes/bin/kubelet \\$KUBELET_OPTSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF 启动并设置开机启动 123systemctl daemon-reloadsystemctl start kubeletsystemctl enable kubelet 6.7 批准kubelet证书申请并加入集群（master执行）1234567891011# 查看kubelet证书请求kubectl get csrNAME AGE SIGNERNAME REQUESTOR CONDITIONnode-csr-uCEGPOIiDdlLODKts8J658HrFq9CZ--K6M4G7bjhk8A 6m3s kubernetes.io/kube-apiserver-client-kubelet kubelet-bootstrap Pendingnode-csr-***# 批准申请kubectl certificate approve node-csr-uCEGPOIiDdlLODKts8J658HrFq9CZ--K6M4G7bjhk8Akubectl certificate approve node-csr-***# 查看节点kubectl get node 由于网络插件还没有部署，节点会没有准备就绪 NotReady。 6.8 部署kube-proxy123456cat &gt; /opt/kubernetes/cfg/kube-proxy.conf &lt;&lt; EOFKUBE_PROXY_OPTS=&quot;--logtostderr=false \\\\--v=2 \\\\--log-dir=/opt/kubernetes/logs \\\\--config=/opt/kubernetes/cfg/kube-proxy-config.yml&quot;EOF 12345678910cat &gt; /opt/kubernetes/cfg/kube-proxy-config.yml &lt;&lt; EOFkind: KubeProxyConfigurationapiVersion: kubeproxy.config.k8s.io/v1alpha1bindAddress: 0.0.0.0metricsBindAddress: 0.0.0.0:10249clientConnection: kubeconfig: /opt/kubernetes/cfg/kube-proxy.kubeconfighostnameOverride: node1clusterCIDR: 10.0.0.0/24EOF hostnameOverride设置对应node机器的hostname。 6.9 生成kube-proxy.kubeconfig文件（master生成传到node）1234567891011121314151617181920212223# 切换工作目录cd TLS/k8s# 创建证书请求文件cat &gt; kube-proxy-csr.json &lt;&lt; EOF&#123; &quot;CN&quot;: &quot;system:kube-proxy&quot;, &quot;hosts&quot;: [], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;ST&quot;: &quot;BeiJing&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125;EOF 12# 生成证书cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy 12[root@master k8s]# ls kube-proxy*pemkube-proxy-key.pem kube-proxy.pem 将master生成的证书传输到node 12scp /root/TLS/k8s/kube-proxy*pem root@192.168.206.129:/opt/kubernetes/sslscp /root/TLS/k8s/kube-proxy*pem root@192.168.206.130:/opt/kubernetes/ssl 6.10 生成kubeconfig文件1KUBE_APISERVER=&quot;https://192.168.206.128:6443&quot; # apiserver IP:PORT 123456789101112131415kubectl config set-cluster kubernetes \\ --certificate-authority=/opt/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=$&#123;KUBE_APISERVER&#125; \\ --kubeconfig=kube-proxy.kubeconfigkubectl config set-credentials kube-proxy \\ --client-certificate=/opt/kubernetes/ssl/kube-proxy.pem \\ --client-key=/opt/kubernetes/ssl/kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfigkubectl config set-context default \\ --cluster=kubernetes \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfigkubectl config use-context default --kubeconfig=kube-proxy.kubeconfig 6.11 systemd管理kube-proxy123456789101112cat &gt; /usr/lib/systemd/system/kube-proxy.service &lt;&lt; EOF[Unit]Description=Kubernetes ProxyAfter=network.target[Service]EnvironmentFile=/opt/kubernetes/cfg/kube-proxy.confExecStart=/opt/kubernetes/bin/kube-proxy \\$KUBE_PROXY_OPTSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF 启动并设置开机启动： 123systemctl daemon-reloadsystemctl start kube-proxysystemctl enable kube-proxy 7.部署CNI网络下载安装 12下载地址：https://github.com/containernetworking/plugins/releases版本：v0.8.6（安装包名：cni-plugins-linux-amd64-v0.8.6.tgz） node节点操作： 12mkdir /opt/cni/bintar zxvf cni-plugins-linux-amd64-v0.8.6.tgz -C /opt/cni/bin master节点操作： 12wget --no-check-certificate https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.ymlkubectl apply -f kube-flannel.yml 参考： 【尚硅谷】Kubernetes（k8s）入门到实战教程丨全新升级完整版 k8s集群 (二进制安装方式)","categories":[{"name":"技术","slug":"技术","permalink":"https://xssdpgy.github.io/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://xssdpgy.github.io/tags/k8s/"},{"name":"安装","slug":"安装","permalink":"https://xssdpgy.github.io/tags/%E5%AE%89%E8%A3%85/"}]}],"categories":[{"name":"技术","slug":"技术","permalink":"https://xssdpgy.github.io/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"队列","slug":"队列","permalink":"https://xssdpgy.github.io/tags/%E9%98%9F%E5%88%97/"},{"name":"timeLimit","slug":"timeLimit","permalink":"https://xssdpgy.github.io/tags/timeLimit/"},{"name":"源码分析","slug":"源码分析","permalink":"https://xssdpgy.github.io/tags/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"name":"seata","slug":"seata","permalink":"https://xssdpgy.github.io/tags/seata/"},{"name":"分布式事务","slug":"分布式事务","permalink":"https://xssdpgy.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"},{"name":"设计模式","slug":"设计模式","permalink":"https://xssdpgy.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"动态代理","slug":"动态代理","permalink":"https://xssdpgy.github.io/tags/%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86/"},{"name":"异步","slug":"异步","permalink":"https://xssdpgy.github.io/tags/%E5%BC%82%E6%AD%A5/"},{"name":"优化","slug":"优化","permalink":"https://xssdpgy.github.io/tags/%E4%BC%98%E5%8C%96/"},{"name":"k8s","slug":"k8s","permalink":"https://xssdpgy.github.io/tags/k8s/"},{"name":"安装","slug":"安装","permalink":"https://xssdpgy.github.io/tags/%E5%AE%89%E8%A3%85/"}]}